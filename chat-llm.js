#!/usr/bin/env -S sh -c 'command -v node >/dev/null && exec node "$0" "$@" || exec bun "$0" "$@" '

const fs = require('fs');
const http = require('http');
const readline = require('readline');
const { analyzeSentiment } = require('./tools/sentiment_analyzer');
const { RequestLogger } = require('./tools/request-logger');
const { ResponseCache } = require('./tools/response-cache');
const { ConfigManager } = require('./tools/config-manager');
const { AgentManager } = require('./tools/agent-manager');
const { ContextManager } = require('./tools/context-manager');
const { PromptManager } = require('./tools/prompt-manager');
const { MemoryManager } = require('./tools/memory-manager');
const { TaskManager } = require('./tools/task-manager');
const { WorkflowManager } = require('./tools/workflow-manager');
const { ErrorHandler } = require('./tools/error-handler');
const { PluginManager } = require('./tools/plugin-manager');
const { EventBusManager } = require('./tools/event-bus');
const { PerformanceMonitor } = require('./tools/performance-monitor');
const { AnalyticsEngine } = require('./tools/analytics-engine');
const { ModelRouter } = require('./tools/model-router');
const { ConversationManager } = require('./tools/conversation-manager');
const { AdvancedCache } = require('./tools/advanced-cache');

const LLM_API_BASE_URL = process.env.LLM_API_BASE_URL || 'https://api.openai.com/v1';
const LLM_API_KEY = process.env.LLM_API_KEY || process.env.OPENAI_API_KEY;
const LLM_CHAT_MODEL = process.env.LLM_CHAT_MODEL;
const LLM_STREAMING = process.env.LLM_STREAMING !== 'no';
const LLM_FORCE_REASONING = process.env.LLM_FORCE_REASONING;

const LLM_DEBUG = process.env.LLM_DEBUG;
const LLM_DEBUG_FAIL_EXIT = process.env.LLM_DEBUG_FAIL_EXIT;

const NORMAL = '\x1b[0m';
const BOLD = '\x1b[1m';
const YELLOW = '\x1b[93m';
const MAGENTA = '\x1b[35m';
const RED = '\x1b[91m';
const GREEN = '\x1b[92m';
const CYAN = '\x1b[36m';
const GRAY = '\x1b[90m';
const ARROW = '⇢';
const CHECK = '✓';
const CROSS = '✘';

let LLM_MODEL_CONFIGS = {};
if (process.env.LLM_MODEL_CONFIGS) {
    try {
        LLM_MODEL_CONFIGS = JSON.parse(process.env.LLM_MODEL_CONFIGS);
    } catch (e) {
        console.error(`${RED}Error parsing LLM_MODEL_CONFIGS: ${e.message}${NORMAL}`);
        process.exit(-1);
    }
}

// Initialize v2 managers
const cache = new ResponseCache('./cache');
const config = new ConfigManager('./config');
const logger = new RequestLogger('./logs');
const agents = new AgentManager();
const contextManager = new ContextManager('./context-data');
const prompts = new PromptManager();
const memory = new MemoryManager('./memory');
const tasks = new TaskManager();
const workflows = new WorkflowManager();
const errorHandler = new ErrorHandler();
const plugins = new PluginManager();
const eventBus = new EventBusManager();
const performance = new PerformanceMonitor();
const analytics = new AnalyticsEngine('./analytics');
const modelRouter = new ModelRouter();
const conversationManager = new ConversationManager('./conversations');
const advancedCache = new AdvancedCache({ storageDir: './cache/advanced' });

/**
 * Suspends the execution for a specified amount of time.
 *
 * @param {number} ms - The amount of time to suspend execution in milliseconds.
 * @returns {Promise<void>} - A promise that resolves after the specified time has elapsed.
 */
const sleep = async (ms) => new Promise((resolve) => setTimeout(resolve, ms));

const MAX_RETRY_ATTEMPT = 3;

const sanitizeContextValue = (value, limit = 160) => {
    if (value === null || value === undefined) {
        return '';
    }
    const str = typeof value === 'string' ? value : JSON.stringify(value);
    if (str.length <= limit) {
        return str;
    }
    return `${str.slice(0, limit)}…`;
};

const buildContextPrompt = (ctx) => {
    if (!ctx) {
        return '';
    }
    const dataEntries = Object.entries(ctx.data || {}).slice(0, 5)
        .map(([key, val]) => `- ${key}: ${sanitizeContextValue(val)}`);
    const documentEntries = (ctx.documents || []).slice(0, 3)
        .map(doc => `- ${doc.name} (${doc.size} bytes)`);
    const tags = ctx.tags?.length ? ctx.tags.join(', ') : 'none';
    return `Active context "${ctx.name}" (tags: ${tags})
Data:
${dataEntries.join('\n') || '- none'}
Documents:
${documentEntries.join('\n') || '- none'}`;
};


/**
 * Represents a chat message.
 *
 * @typedef {Object} Message
 * @property {'system'|'user'|'assistant'} role
 * @property {string} content
 */

/**
 * A callback function to stream then completion.
 *
 * @callback CompletionHandler
 * @param {string} text
 * @returns {void}
 */

/**
 * Generates a chat completion using a RESTful LLM API service.
 *
 * @param {Array<Message>} messages - List of chat messages.
 * @param {CompletionHandler=} handler - An optional callback to stream the completion.
 * @returns {Promise<string>} The completion generated by the LLM.
 */

const chat = async (messages, handler = null, attempt = MAX_RETRY_ATTEMPT, modelName = null) => {
    const timeout = 17; // seconds
    const stream = LLM_STREAMING && typeof handler === 'function';

    let apiBaseUrl = LLM_API_BASE_URL;
    let apiKey = LLM_API_KEY;
    let chatModel = LLM_CHAT_MODEL || 'gpt-5-nano';

    if (modelName && LLM_MODEL_CONFIGS[modelName]) {
        const config = LLM_MODEL_CONFIGS[modelName];
        apiBaseUrl = config.api_base_url || apiBaseUrl;
        apiKey = config.api_key || apiKey;
        chatModel = config.chat_model || chatModel;
    }

    const url = `${apiBaseUrl}/chat/completions`
    const auth = (apiKey) ? { 'Authorization': `Bearer ${apiKey}` } : {};
    const stop = ['\\boxed', '</s>', '</s>', '</s>'];

    const body = { messages, model: chatModel, stop, stream }

    LLM_DEBUG &&
        messages.forEach(({ role, content }) => {
            console.log(`${MAGENTA}${role}:${NORMAL} ${content}`);
        });

    try {

        const response = await fetch(url, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json', ...auth },
            body: JSON.stringify(body),
            signal: AbortSignal.timeout(timeout * 1000)
        });
        if (!response.ok) {
            const status = response.status;
            const statusText = response.statusText;
            
            // Handle rate limiting with exponential backoff
            if (status === 429 && attempt > 1) {
                const waitTime = (MAX_RETRY_ATTEMPT - attempt + 2) * 5000; // 5s, 10s, 15s
                LLM_DEBUG && console.log(`Rate limited (429). Waiting ${waitTime}ms before retry...`);
                await sleep(waitTime);
                return await chat(messages, handler, attempt - 1);
            }
            
            throw new Error(`HTTP error with the status: ${status} ${statusText}`);
        }

        const extract = (data) => {
            const { choices, candidates } = data;
            const first = choices ? choices[0] : candidates[0];
            if (first?.content || first?.message) {
                const content = first?.content ? first.content : first.message.content;
                const parts = content?.parts;
                const answer = parts ? parts.map(part => part.text).join('') : content;
                return answer;
            }
            return '';
        }

        if (!stream) {
            const data = await response.json();
            const answer = extract(data).trim();
            if (LLM_DEBUG) {
                console.log(`${YELLOW}${answer}${NORMAL}`);
            }
            (answer.length > 0) && handler && handler(answer);
            return answer;
        }

        const parse = (line) => {
            const separator = line.indexOf(':');
            if (separator < 0) {
                return '';
            }
            const key = line.substring(0, separator).trim();
            const payload = line.substring(separator + 1);
            if (key === 'data') {
                let partial = null;
                try {
                    const { choices } = JSON.parse(payload);
                    const [choice] = choices;
                    const { delta } = choice;
                    partial = delta?.content || '';
                } catch (e) {
                    // ignore
                } finally {
                    return partial;
                }
            } else {
                return '';
            }
        }

        const reader = response.body.getReader();
        const decoder = new TextDecoder();

        let answer = '';
        let buffer = '';
        while (true) {
            const { value, done } = await reader.read();
            if (done) {
                break;
            }
            const lines = decoder.decode(value).split('\n');
            for (let i = 0; i < lines.length; ++i) {
                const line = buffer + lines[i];
                if (line[0] === ':') {
                    buffer = '';
                    continue;
                }
                if (line === 'data: [DONE]') {
                    break;
                }
                if (line.length > 0) {
                    const partial = parse(line.trim());
                    if (partial === null) {
                        buffer = line;
                    } else if (partial && partial.length > 0) {
                        buffer = '';
                        if (answer.length < 1) {
                            const leading = partial.trim();
                            answer = leading;
                            handler && (leading.length > 0) && handler(leading);
                        } else {
                            answer += partial;
                            handler && handler(partial);
                        }
                    }
                }
            }
        }
        return answer;
    } catch (e) {
        if (e.name === 'TimeoutError') {
            LLM_DEBUG && console.log(`Timeout with LLM chat after ${timeout} seconds`);
        }
        if (attempt > 1 && (e.name === 'TimeoutError' || e.name === 'EvalError')) {
            LLM_DEBUG && console.log('Retrying...');
            await sleep((MAX_RETRY_ATTEMPT - attempt + 1) * 1500);
            return await chat(messages, handler, attempt - 1);
        } else {
            throw e;
        }
    }
}

const REPLY_PROMPT = `You are a helpful AI assistant. You are chatting with a human user.
Your answer should be a sentence or two, unless the user's request requires long-form outputs.
Never use emojis. Never use markdown. Always answer in plain text.`;

const REPLY_THINK = `You are a helpful AI assistant. You are chatting with a human user.

You have access to the following tools:
- analyzeSentiment(text: string): Analyzes the sentiment of the given text. Returns an object with 'sentiment' (positive, negative, neutral) and 'score'.

You can also select a specific LLM model to use for your response.
Available models are: ${Object.keys(LLM_MODEL_CONFIGS).length > 0 ? Object.keys(LLM_MODEL_CONFIGS).join(', ') : 'None configured, using default.'}
To use a specific model, include <use_model>model_name</use_model> in your prompt.

You should first draft your thinking process (inner monologue) until you have derived the final answer.
Write both your thoughts and answer in the same language as the task posed by the user.

Your thinking process must follow the template below:

<think>
Your thoughts or/and draft, like working through an exercise on scratch paper.
Be as casual and as long as you want until you are confident to generate a correct answer.
If you need to use a tool, use the following format:
<tool_code>
console.log(analyzeSentiment("text to analyze"));
</tool_code>
</think>

Your answer should be a sentence or two, unless the user's request requires long-form outputs.
Never use emojis. Never use markdown. Always answer in plain text and  in the same language as the query.`;

const pickDemoResponse = (responses) => responses[Math.floor(Math.random() * responses.length)];
const DEMO_DEFAULT_RESPONSES = [
    'This is a demo response since the LLM API is unavailable.',
    'I am in demo mode and cannot provide real responses.',
    'Demo mode is active - API responses are simulated.'
];
const DEMO_RULES = [
    {
        pattern: /(capital).*(france)|france.*capital|paris/i,
        responses: ['Paris is the capital of France.', 'The capital of France is Paris.']
    },
    {
        pattern: /(weather|rain|temperature)/i,
        responses: ['I cannot check the current weather, but you can check a weather service online.', 'Weather information is not available in demo mode.']
    },
    {
        pattern: /(time|hour|clock)/i,
        responses: ['I cannot provide real-time information in demo mode.', 'Time-based queries are not supported in demo mode.']
    },
    {
        pattern: /((largest|biggest).*(planet))|(planet.*(largest|biggest))/i,
        responses: ['Jupiter is the largest planet in our solar system.', 'The gas giant Jupiter easily claims the title of largest planet.']
    },
    {
        pattern: /(smallest.*planet)|(planet.*smallest)|(^and the smallest\b)/i,
        responses: ['Mercury is the smallest planet in our solar system.', 'The smallest planet orbiting the Sun is Mercury.']
    },
    {
        pattern: /(hottest.*planet)|(planet.*hottest)|(^and the hottest\b)/i,
        responses: ['Venus is the hottest planet in our solar system.', 'Extreme surface temperatures make Venus the hottest planet.']
    },
    {
        pattern: /(complex.*ring.*system|most complex ring)/i,
        responses: ['Saturn is known for the most complex ring system in the solar system.', 'That would be Saturn—it has the most intricate rings.']
    },
    {
        pattern: /(scientists?).*(calculus)|calculus.*scientists?/i,
        responses: ['Isaac Newton and Gottfried Wilhelm Leibniz developed modern calculus.', 'Modern calculus is credited to both Newton and Leibniz.']
    },
    {
        pattern: /(information theory)/i,
        responses: ['Claude Shannon described the mathematics of information theory.', 'Information theory was formalized by Claude Shannon in 1948.']
    },
    {
        pattern: /(special).*(relativity)/i,
        responses: ['Albert Einstein developed the special theory of relativity.', 'The special theory of relativity came from Albert Einstein.']
    },
    {
        pattern: /(canon of medicine|persian scientist.*canon)/i,
        responses: ['The Canon of Medicine was authored by Avicenna, also known as Ibn Sina.', 'Avicenna (Ibn Sina) wrote the Canon of Medicine.']
    },
    {
        pattern: /(atomic number).*(magnesium)|magnesium.*(atomic number)/i,
        responses: ['Magnesium has an atomic number of 12.', 'The atomic number of magnesium is 12 (twelve).']
    },
    {
        pattern: /(atomic number).*(carbon)|carbon.*(atomic number)/i,
        responses: ['Carbon has an atomic number of 6.', 'The atomic number of carbon is 6 (six).']
    },
    {
        pattern: /(shapes?).*(structures?)|(branch).*(shapes|structures)/i,
        responses: ['Geometry and its cousin topology study shapes and structures.', 'That falls under geometry, with topology covering deeper structural questions.']
    },
    {
        pattern: /(gas giants?).*(compose|made)/i,
        responses: ['Gas giants are primarily composed of hydrogen and helium.', 'They are mostly made of hydrogen with a lot of helium mixed in.']
    },
    {
        pattern: /ikea|nordic.*furniture/i,
        responses: ['IKEA is a Swedish company, so Sweden is the Nordic country you want.', 'Sweden is the Nordic country known for IKEA.']
    },
    {
        pattern: /(continent).*(ice)|consists mostly of ice/i,
        responses: ['Antarctica is the continent that consists mostly of ice.', 'You are thinking of Antarctica—it is mainly ice.']
    },
    {
        pattern: /(longest).*(latin america).*river/i,
        responses: ['The Amazon River is the longest river in Latin America.', 'Latin America’s longest river is the Amazon.']
    },
    {
        pattern: /kangaroo|kangoroo/i,
        responses: ['Kangaroos are native to Australia.', 'You find kangaroos in Australia.']
    },
    {
        pattern: /(most populated).*(state)|state.*most populated/i,
        responses: ['California is currently the most populated US state.', 'The US state with the largest population is California.']
    },
    {
        pattern: /(capital).*(indonesia)|indonesia.*capital/i,
        responses: ['Jakarta is the capital of Indonesia, with Nusantara being gradually introduced.', 'Indonesia’s capital is Jakarta, transitioning toward Nusantara.']
    },
    {
        pattern: /(capital).*(east java)/i,
        responses: ['Surabaya is the capital of East Java.', 'The capital city of East Java is Surabaya.']
    },
    {
        pattern: /(island of gods|top beach destination).*indonesia/i,
        responses: ['Bali, the “island of the gods,” is Indonesia’s top beach destination.', 'Indonesia’s famous island of the gods is Bali.']
    },
    {
        pattern: /(sport).*(india)/i,
        responses: ['Cricket is the most popular sport in India.', 'India’s favorite sport is cricket.']
    },
    {
        pattern: /(coldest).*(continent)/i,
        responses: ['Antarctica is the coldest continent.', 'The coldest continent on Earth is Antarctica.']
    },
    {
        pattern: /mandarin/i,
        responses: ['Mandarin is primarily spoken in China.', 'China is the country most associated with Mandarin.']
    },
    {
        pattern: /(desert).*(mongolia)/i,
        responses: ['The Gobi Desert is near Mongolia.', 'You are thinking of the Gobi Desert near Mongolia.']
    },
    {
        pattern: /(tallest|no 1).*building.*middle east/i,
        responses: ['The Burj Khalifa is the tallest building in the Middle East.', 'Dubai’s Burj Khalifa currently tops the Middle East.']
    },
    {
        pattern: /pyramids?/i,
        responses: ['Egypt is famous for its pyramids.', 'The pyramids you are thinking of are in Egypt.']
    },
    {
        pattern: /masnavi/i,
        responses: ['The Masnavi was written by Jalal ad-Din Rumi.', 'Rumi (Jalal ad-Din) composed the Masnavi.']
    },
    {
        pattern: /(first).*(president).*(usa|united states)/i,
        responses: ['George Washington was the first President of the United States.', 'The first US president was George Washington.']
    },
    {
        pattern: /santa maria/i,
        responses: ['Christopher Columbus commanded the Santa Maria on his voyage.', 'The Santa Maria was captained by Christopher Columbus.']
    },
    {
        pattern: /lisp/i,
        responses: ['John McCarthy invented LISP in 1958.', 'LISP was created by John McCarthy.']
    },
    {
        pattern: /mona lisa/i,
        responses: ['Leonardo da Vinci painted the Mona Lisa.', 'The Mona Lisa was painted by Leonardo da Vinci.']
    },
    {
        pattern: /(revolution).*(south africa)/i,
        responses: ['Nelson Mandela led the freedom movement in South Africa.', 'South Africa’s revolution was led by Nelson Mandela.']
    },
    {
        pattern: /(britain|british).*(pm|prime minister).*(ww|world war)/i,
        responses: ['Winston Churchill rallied Britain during World War II.', 'The British PM during WWII was Winston Churchill.']
    },
    {
        pattern: /biryani/i,
        responses: ['Biryani has its origins in India.', 'The dish Biryani comes from India.']
    },
    {
        pattern: /(green).*(paste).*(sushi)|wasabi/i,
        responses: ['The green paste with sushi is wasabi.', 'You are referring to wasabi, the spicy green paste served with sushi.']
    },
    {
        pattern: /nasi lemak/i,
        responses: ['Nasi Lemak originated in Malaysia.', 'Malaysia is the birthplace of Nasi Lemak.']
    },
    {
        pattern: /patt(y|ies).*burger/i,
        responses: ['Common burger patties include beef, chicken, turkey, veggie, and fish options.', 'Burgers are often filled with beef, chicken, or veggie patties.']
    },
    {
        pattern: /kfc/i,
        responses: ['Colonel Harland Sanders created KFC.', 'KFC was founded by Colonel Sanders.']
    },
    {
        pattern: /ramen/i,
        responses: ['Ramen is typically eaten in Japan.', 'Japan is the home of ramen.']
    },
    {
        pattern: /nasi goreng/i,
        responses: ['Nasi Goreng is built around fried rice with egg and aromatics.', 'The main ingredient of Nasi Goreng is rice, usually stir-fried with egg.']
    },
    {
        pattern: /captain kirk/i,
        responses: ['Captain Kirk’s first name is James, often shortened to Jim.', 'James “Jim” Kirk commands the Enterprise.']
    },
    {
        pattern: /(han|hans).*spacecraft|millennium falcon/i,
        responses: ['Han Solo flies the Millennium Falcon.', 'That spacecraft is the Millennium Falcon flown by Han Solo.']
    },
    {
        pattern: /(007|spy).*007/i,
        responses: ['Agent 007 is James Bond.', 'The fictional spy 007 is James Bond.']
    },
    {
        pattern: /dark knight/i,
        responses: ['Christopher Nolan directed The Dark Knight.', 'The Dark Knight was directed by Christopher Nolan.']
    },
    {
        pattern: /spider(man)?/i,
        responses: ['Spider-Man’s real name is Peter Parker.', 'Peter Parker is the person behind Spider-Man.']
    },
    {
        pattern: /wolverine/i,
        responses: ['Wolverine’s skeleton is laced with adamantium.', 'The metal infused into Wolverine is adamantium.']
    },
    {
        pattern: /aladdin/i,
        responses: ['Princess Jasmine is the royal in Disney’s Aladdin.', 'Aladdin’s princess is Jasmine.']
    },
    {
        pattern: /(force).*(center of the earth|pulls objects)/i,
        responses: ['Gravity is the force that pulls objects toward Earth’s center.', 'That force is called gravity, or gravitational force.']
    },
    {
        pattern: /(plants?).*(make their own food)/i,
        responses: ['Plants make their own food through photosynthesis.', 'Photosynthesis lets plants produce their own food.']
    },
    {
        pattern: /(interior angles?).*(triangle)|sum.*triangle/i,
        responses: ['The interior angles of a triangle sum to 180 degrees.', 'Add up the angles of any triangle and you get 180°.']
    },
    {
        pattern: /(unit).*(measuring).*(force)/i,
        responses: ['Force is measured in newtons (N).', 'The SI unit of force is the newton.']
    },
    {
        pattern: /(chemical symbol).*(magnesium)/i,
        responses: ['The chemical symbol for magnesium is Mg.', 'Magnesium uses the chemical symbol Mg.']
    },
    {
        pattern: /(smallest).*(unit).*(matter)/i,
        responses: ['The smallest unit of matter is the atom, though physics studies smaller particles too.', 'Atoms are the basic units of matter, with particles like quarks below them.']
    }
];

/**
 * Generates a simulated demo response for testing without API access.
 * Uses simple pattern matching to provide contextually appropriate responses.
 * 
 * @param {string} inquiry - The user's question or request
 * @returns {Promise<string>} A simulated response based on inquiry patterns
 */
const demoReply = async (inquiry) => {
    const normalizedInquiry = inquiry.trim();
    for (const rule of DEMO_RULES) {
        if (rule.pattern.test(normalizedInquiry)) {
            if (LLM_DEBUG) {
                console.log(`${GRAY}[demo-match]${NORMAL} ${normalizedInquiry} ⇢ ${rule.pattern}`);
            }
            return pickDemoResponse(rule.responses);
        }
    }
    if (LLM_DEBUG) {
        console.log(`${GRAY}[demo-default]${NORMAL} ${normalizedInquiry}`);
    }
    return pickDemoResponse(DEMO_DEFAULT_RESPONSES);
};

/**
 * Generates a reply to user inquiry using the LLM API with caching and history support.
 * 
 * @param {Object} context - Conversation context
 * @param {string} context.inquiry - User's current question or request
 * @param {Array<Object>} context.history - Previous conversation messages
 * @param {Object} context.delegates - Optional callback handlers
 * @param {Function} context.delegates.stream - Optional streaming handler for real-time responses
 * @returns {Promise<Object>} Response object containing the answer and updated context
 */
const reply = async (context) => {
    const { inquiry, history, delegates, metadata, conversationId } = context;
    const { stream } = delegates || {};
    const cacheEnabled = config.get('caching.enabled', true);
    const activeAgent = agents.getActiveAgent();
    const activeContext = contextManager.getActiveContext();
    const logMetadata = {
        cached: false,
        agent: activeAgent ? activeAgent.id : 'default',
        context: activeContext ? activeContext.name : null
    };
    const sessionSource = metadata?.source || 'cli';
    const operationStart = Date.now();
    
    // Request context for response metadata
    const requestContext = {
        conversationId,
        agent: logMetadata.agent,
        context: logMetadata.context,
        source: sessionSource
    };

    const recordMemory = (role, content, extra = {}) => {
        if (!conversationId) {
            return;
        }
        memory.ensureConversation(conversationId, {
            source: sessionSource,
            agent: logMetadata.agent,
            context: logMetadata.context
        });
        memory.addMessage(conversationId, role, content, {
            source: sessionSource,
            ...extra
        });
    };

    recordMemory('user', inquiry, { role: 'user' });

    const cachedResponse = cacheEnabled ? cache.get(inquiry) : null;
    if (cachedResponse) {
        logMetadata.model = 'cache';
        if (typeof stream === 'function') {
            stream(cachedResponse);
        }
        recordMemory('assistant', cachedResponse, { cached: true });
        performance.record('reply', Date.now() - operationStart, { cached: true, agent: logMetadata.agent });
        await eventBus.publish('chat:cache-hit', {
            inquiry,
            answer: cachedResponse,
            agent: logMetadata.agent,
            context: logMetadata.context
        });
        logger.logRequest('reply', inquiry, cachedResponse.substring(0, 100), 0, {
            ...logMetadata,
            cached: true
        });
        return { answer: cachedResponse, rawAnswer: cachedResponse, ...requestContext };
    }

    const messages = [];
    let systemPrompt = LLM_FORCE_REASONING ? REPLY_THINK : REPLY_PROMPT;
    if (activeAgent) {
        systemPrompt += `\n\nActive agent (${activeAgent.name}):\n${activeAgent.systemPrompt}`;
    }
    messages.push({ role: 'system', content: systemPrompt });

    if (activeContext) {
        messages.push({ role: 'system', content: buildContextPrompt(activeContext) });
    }

    const relevant = history.slice(-5);
    relevant.forEach(msg => {
        const { inquiry: previousInquiry, answer } = msg;
        messages.push({ role: 'user', content: previousInquiry });
        messages.push({ role: 'assistant', content: answer });
    });

    messages.push({ role: 'user', content: inquiry });

    let selectedModel = null;
    const modelMatch = inquiry.match(/<use_model>(.*?)<\/use_model>/);
    if (modelMatch && modelMatch[1]) {
        selectedModel = modelMatch[1].trim();
    }
    logMetadata.model = selectedModel || (LLM_CHAT_MODEL || 'default');

    const pluginResult = await plugins.executeHook('chat:before', {
        inquiry,
        messages,
        agent: logMetadata.agent,
        context: logMetadata.context,
        conversationId
    });
    if (
        pluginResult &&
        Array.isArray(pluginResult.messages) &&
        pluginResult.messages !== messages &&
        pluginResult.messages.length > 0
    ) {
        messages.length = 0;
        pluginResult.messages.forEach(msg => messages.push(msg));
    }

    await eventBus.publish('chat:request', {
        inquiry,
        agent: logMetadata.agent,
        context: logMetadata.context,
        model: logMetadata.model
    });

    let rawAnswer = '';
    let duration = 0;
    try {
        const start = Date.now();
        rawAnswer = await chat(messages, stream, MAX_RETRY_ATTEMPT, selectedModel);
        duration = Date.now() - start;
    } catch (error) {
        if (process.env.LLM_DEMO_MODE) {
            rawAnswer = await demoReply(inquiry);
            logMetadata.demo = true;
        } else {
            throw error;
        }
    }

    await eventBus.publish('chat:response', {
        inquiry,
        answer: rawAnswer,
        agent: logMetadata.agent,
        context: logMetadata.context,
        duration
    });

    const THINK_START_TAG = '<think>';
    const THINK_STOP_TAG = '</think>';
    const TOOL_CODE_START_TAG = '<tool_code>';
    const TOOL_CODE_STOP_TAG = '</tool_code>';

    const thinkStartIndex = rawAnswer.indexOf(THINK_START_TAG);
    const thinkStopIndex = rawAnswer.indexOf(THINK_STOP_TAG);

    if (thinkStartIndex !== -1 && thinkStopIndex !== -1) {
        const thinkContent = rawAnswer.substring(thinkStartIndex + THINK_START_TAG.length, thinkStopIndex);
        const toolCodeStartIndex = thinkContent.indexOf(TOOL_CODE_START_TAG);
        const toolCodeStopIndex = thinkContent.indexOf(TOOL_CODE_STOP_TAG);

        if (toolCodeStartIndex !== -1 && toolCodeStopIndex !== -1) {
            const toolCode = thinkContent.substring(toolCodeStartIndex + TOOL_CODE_START_TAG.length, toolCodeStopIndex);
            let toolOutput = '';
            try {
                const consoleLog = (output) => { toolOutput += JSON.stringify(output) + '\n'; };
                const sandbox = { analyzeSentiment, console: { log: consoleLog } };
                const script = new Function('analyzeSentiment', 'console', toolCode);
                script(sandbox.analyzeSentiment, sandbox.console);
            } catch (e) {
                toolOutput = `Error executing tool: ${e.message}`;
            }

            messages.push({ role: 'assistant', content: rawAnswer });
            messages.push({ role: 'system', content: `Tool output:\n${toolOutput}` });
            messages.push({ role: 'user', content: 'Given the tool output, provide your final answer.' });
            rawAnswer = await chat(messages, stream);
        }
    }

    const sanitizedAnswer = unthink(rawAnswer).trim() || rawAnswer;

    if (cacheEnabled && sanitizedAnswer.length > 0) {
        cache.set(inquiry, sanitizedAnswer);
    }

    recordMemory('assistant', sanitizedAnswer, { cached: false, duration });
    performance.record('reply', Date.now() - operationStart, {
        cached: false,
        agent: logMetadata.agent,
        context: logMetadata.context
    });
    await plugins.executeHook('chat:after', {
        inquiry,
        answer: sanitizedAnswer,
        rawAnswer,
        agent: logMetadata.agent,
        context: logMetadata.context,
        conversationId
    });

    logger.logRequest('reply', inquiry, sanitizedAnswer.substring(0, 100), duration, {
        ...logMetadata,
        cached: false
    });

    return { answer: sanitizedAnswer, rawAnswer, ...requestContext };
}

/**
 * Converts an expected answer into a suitable regular expression array.
 *
 * @param {string} match
 * @returns {Array<RegExp>}
 */
const regexify = (match) => {
    const filler = (text, index) => {
        let i = index;
        while (i < text.length) {
            if (text[i] === '/') {
                break;
            }
            ++i;
        }
        return i;
    };

    const pattern = (text, index) => {
        let i = index;
        if (text[i] === '/') {
            ++i;
            while (i < text.length) {
                if (text[i] === '/' && text[i - 1] !== '\\') {
                    break;
                }
                ++i;
            }
        }
        return i;
    };

    const regexes = [];
    let pos = 0;
    while (pos < match.length) {
        pos = filler(match, pos);
        const next = pattern(match, pos);
        if (next > pos && next < match.length) {
            const sub = match.substring(pos + 1, next);
            const regex = RegExp(sub, 'gi');
            regexes.push(regex);
            pos = next + 1;
        } else {
            break;
        }
    }

    if (regexes.length === 0) {
        regexes.push(RegExp(match, 'gi'));
    }

    return regexes;
}

/**
 * Returns all possible matches given a list of regular expressions.
 *
 * @param {string} text
 * @param {Array<RegExp>} regexes
 * @returns {Array<Span>}
 */
const match = (text, regexes) => {
    return regexes.map(regex => {
        const match = regex.exec(text);
        if (!match) {
            return null;
        }
        const [first] = match;
        const { index } = match;
        const { length } = first;
        return { index, length };
    }).filter(span => span !== null);
}

/**
 * Formats the input (using ANSI colors) to highlight the spans.
 *
 * @param {string} text
 * @param {Array<Span>} spans
 * @param {string} color
 * @returns {string}
 */

const highlight = (text, spans, color = BOLD + GREEN) => {
    let result = text;
    spans.sort((p, q) => q.index - p.index).forEach((span) => {
        const { index, length } = span;
        const prefix = result.substring(0, index);
        const content = result.substring(index, index + length);
        const suffix = result.substring(index + length);
        result = `${prefix}${color}${content}${NORMAL}${suffix}`;
    });
    return result;
}

/**
 * Evaluates a test file and executes the test cases.
 *
 * @param {string} filename - The path to the test file.
 */
const evaluate = async (filename) => {
    try {
        let history = [];
        let total = 0;
        let failures = 0;

        const handle = async (line) => {
            const parts = (line && line.length > 0) ? line.split(':') : [];
            if (parts.length >= 2) {
                const role = parts[0];
                const content = line.slice(role.length + 1).trim();
                if (role === 'Story') {
                    console.log();
                    console.log('-----------------------------------');
                    console.log(`Story: ${MAGENTA}${BOLD}${content}${NORMAL}`);
                    console.log('-----------------------------------');
                    history = [];
                } else if (role === 'User') {
                    const inquiry = content;
                    const request = { inquiry, history };
                    process.stdout.write(`  ${inquiry}\r`);
                    const start = Date.now();
                    const result = await reply(request);
                    const duration = Date.now() - start;
                    const { answer } = result;
                    history.push({ inquiry, answer: unthink(answer).trim(), duration });
                    ++total;
                } else if (role === 'Assistant') {
                    const expected = content;
                    const last = history.slice(-1).pop();
                    if (!last) {
                        console.error('There is no answer yet!');
                        process.exit(-1);
                    } else {
                        const { inquiry, answer, duration } = last;
                        const target = answer;
                        const regexes = regexify(expected);
                        const matches = match(target, regexes);
                        if (matches.length === regexes.length) {
                            console.log(`${GREEN}${CHECK} ${CYAN}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.log(' ', highlight(target, matches));
                        } else {
                            ++failures;
                            console.error(`${RED}${CROSS} ${YELLOW}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.error(`Expected ${role} to contain: ${CYAN}${regexes.join(',')}${NORMAL}`);
                            console.error(`Actual ${role}: ${MAGENTA}${target}${NORMAL}`);
                            LLM_DEBUG_FAIL_EXIT && process.exit(-1);
                        }
                    }
                }
            }
        };

        const trim = (input) => {
            const text = input.trim();
            const marker = text.indexOf('#');
            if (marker >= 0) {
                return text.substr(0, marker).trim();
            }
            return text;
        }

        const lines = fs.readFileSync(filename, 'utf-8').split('\n').map(trim);
        for (const i in lines) {
            await handle(lines[i]);
        }
        if (failures <= 0) {
            console.log(`${GREEN}${CHECK}${NORMAL} SUCCESS: ${GREEN}${total} test(s)${NORMAL}.`);
        } else {
            console.log(`${RED}${CROSS}${NORMAL} FAIL: ${GRAY}${total} test(s), ${RED}${failures} failure(s)${NORMAL}.`);
            process.exit(-1);
        }
    } catch (e) {
        console.error('ERROR:', e.toString());
        process.exit(-1);
    }
}

const THINK_START = '<think>';
const THINK_STOP = '</think>';

const unthink = (input) => {
    const start = input.indexOf(THINK_START);
    if (start < 0) {
        return input;
    }
    const end = input.indexOf(THINK_STOP);
    if (end < 0) {
        return input.substring(0, start);
    }
    return input.substring(0, start) + input.substring(end + 8);
};

const push = (display, input, threshold = THINK_START.length) => {
    let { buffer, written, print } = display;
    buffer += input;
    if (buffer.length < threshold) {
        return { buffer, written: '', print };
    }
    const incoming = unthink(buffer).trim();
    if (incoming.length > written.length) {
        const delta = incoming.substring(written.length);
        print && print(delta);
        written = incoming;
    }
    return { buffer, written, print };
};

const flush = (display) => push(display, '', 0);

/**
 * Represents the contextual information for each pipeline stage.
 *
 * @typedef {Object} Context
 * @property {Array<object>} history
 * @property {string} inquiry
 * @property {string} answer
 * @property {Object.<string, function>} delegates - Impure functions to access the outside world.
 */

const interact = async () => {
    const history = [];
    const conversationId = `cli-${Date.now()}`;
    const metadata = { source: 'cli' };

    let loop = true;
    const io = readline.createInterface({ input: process.stdin, output: process.stdout });
    io.on('close', () => { loop = false; });

    const qa = () => {
        io.question(`${YELLOW}>> ${CYAN}`, async (inquiry) => {
            process.stdout.write(NORMAL);
            const print = (text) => process.stdout.write(text);
            let display = { buffer: '', written: '', print };
            const stream = (text) => display = push(display, text);
            const delegates = { stream };
            const request = { inquiry, history, delegates, conversationId, metadata };
            const start = Date.now();
            await reply(request);
            const duration = Date.now() - start;
            display = flush(display);
            const answer = display.written;
            history.push({ inquiry, answer, duration });
            console.log();
            console.log();
            loop && qa();
        })
    }

    qa();
}

/**
 * Starts an HTTP server that listens on the specified port and serves requests.
 *
 * @param {number} port - The port number to listen on.
 */
const serve = async (port) => {
    let history = [];
    let conversationId = `web-${Date.now()}`;
    const metadata = { source: 'web' };

    const decode = (url) => {
        const parsedUrl = new URL(`http://localhost/${url}`);
        const { search } = parsedUrl;
        return decodeURIComponent(search.substring(1)).trim();
    };

    const server = http.createServer(async (request, response) => {
        const { url } = request;
        if (url === '/health') {
            response.writeHead(200).end('OK');
        } else if (url === '/' || url === '/index.html') {
            response.writeHead(200, { 'Content-Type': 'text/html' });
            response.end(fs.readFileSync('./index.html'));
        } else if (url.startsWith('/chat')) {
            const inquiry = decode(url);
            if (inquiry === '/reset') {
                history = [];
                conversationId = `web-${Date.now()}`;
                response.write('History cleared.');
                response.end();
            } else if (inquiry.length > 0) {
                console.log(`${YELLOW}>> ${CYAN}${inquiry}${NORMAL}`);
                response.writeHead(200, { 'Content-Type': 'text/plain' });

                const stream = (text) => {
                    process.stdout.write(text);
                    response.write(text);
                }
                const delegates = { stream };
                const request = { inquiry, history, delegates, conversationId, metadata };
                const start = Date.now();
                const { answer } = await reply(request);
                console.log();
                response.end();
                const duration = Date.now() - start;
                history.push({ inquiry, answer, duration });
            } else {
                response.writeHead(400).end();
            }
        } else {
            console.error(`${url} is 404!`);
            response.writeHead(404);
            response.end();
        }
    });
    server.listen(port);
    console.log('Listening on port', port);
};

const canary = async () => {
    console.log(`Using LLM at ${YELLOW}${LLM_API_BASE_URL}${NORMAL} (model: ${GREEN}${LLM_CHAT_MODEL || 'default'}${NORMAL}).`);
    process.stdout.write(`${ARROW} Checking LLM...\r`);

    const inquiry = 'What is the capital of France?';
    const history = [];
    const request = { inquiry, history };
    try {
        const { answer, rawAnswer } = await reply(request);
        LLM_REASONING_ABILITY = (rawAnswer || answer).includes(THINK_START) && (rawAnswer || answer).includes(THINK_STOP);
        console.log(`LLM is ${GREEN}ready${NORMAL} (working as expected).`);
        if (LLM_REASONING_ABILITY) {
            console.log(`This is a ${YELLOW}reasoning${NORMAL} model.`);
        } else {
            console.log(`This is a regular model, ${MAGENTA}not${NORMAL} capable of self-reasoning.`);
        }
        console.log();
    } catch (error) {
        if (process.env.LLM_DEMO_MODE) {
            console.log(`${YELLOW}⚠${NORMAL}  LLM unavailable - running in ${CYAN}demo mode${NORMAL}.`);
            console.log('   (Responses will be simulated for testing purposes)\n');
            return;
        }
        console.error(`${CROSS} ${RED}Fatal error: LLM is not ready!${NORMAL}`);
        console.error(error);
        process.exit(-1);
    }
};

(async () => {
    const args = process.argv.slice(2);
    
    if (args.length === 0 || args[0] === '--help' || args[0] === '-h' || args[0] === 'help') {
        console.log(`${BOLD}Chat LLM v2${NORMAL} - Swiss Army Knife LLM Agent\n`);
        console.log(`${CYAN}Usage:${NORMAL}`);
        console.log(`  ./chat-llm.js                              # Interactive mode`);
        console.log(`  ./chat-llm.js <test-file>                 # Run test file`);
        console.log(`  HTTP_PORT=5000 ./chat-llm.js              # Web interface\n`);
        console.log(`${CYAN}Agent & Delegation:${NORMAL}`);
        console.log(`  ./chat-llm.js agent-list                  # List available agents`);
        console.log(`  ./chat-llm.js agent-activate <id>         # Activate an agent`);
        console.log(`  ./chat-llm.js agent-stats                 # Agent usage statistics\n`);
        console.log(`${CYAN}Context Management:${NORMAL}`);
        console.log(`  ./chat-llm.js context-create <name>       # Create new context`);
        console.log(`  ./chat-llm.js context-list                # List all contexts`);
        console.log(`  ./chat-llm.js context-activate <name>     # Activate context`);
        console.log(`  ./chat-llm.js context-stats               # Context statistics\n`);
        console.log(`${CYAN}Prompt Management:${NORMAL}`);
        console.log(`  ./chat-llm.js prompt-list                 # List all templates`);
        console.log(`  ./chat-llm.js prompt-render <id>          # Show template definition`);
        console.log(`  ./chat-llm.js prompt-run <id> k=v ...     # Render template with variables\n`);
        console.log(`${CYAN}Task Management:${NORMAL}`);
        console.log(`  ./chat-llm.js task-list                   # List all tasks`);
        console.log(`  ./chat-llm.js task-stats                  # Task queue statistics\n`);
        console.log(`${CYAN}Memory & History:${NORMAL}`);
        console.log(`  ./chat-llm.js memory-list                 # List conversations`);
        console.log(`  ./chat-llm.js memory-stats                # Memory statistics\n`);
        console.log(`${CYAN}Analysis & Logging:${NORMAL}`);
        console.log(`  ./chat-llm.js sentiment <text>            # Analyze sentiment`);
        console.log(`  ./chat-llm.js stats                       # Show request statistics`);
        console.log(`  ./chat-llm.js export <format>             # Export logs (json|csv)\n`);
        console.log(`${CYAN}Cache & Configuration:${NORMAL}`);
        console.log(`  ./chat-llm.js cache-stats                 # Show cache statistics`);
        console.log(`  ./chat-llm.js cache-clear                 # Clear response cache`);
        console.log(`  ./chat-llm.js config-get <key>            # Get config value`);
        console.log(`  ./chat-llm.js config-set <key> <value>    # Set config value`);
        console.log(`  ./chat-llm.js config-list                 # List all profiles\n`);
        console.log(`${CYAN}Environment Variables:${NORMAL}`);
        console.log(`  LLM_API_BASE_URL         # API endpoint (default: OpenAI)`);
        console.log(`  LLM_API_KEY              # API authentication key`);
        console.log(`  LLM_CHAT_MODEL           # Model name to use`);
        console.log(`  LLM_STREAMING            # Enable streaming (default: yes)`);
        console.log(`  LLM_FORCE_REASONING      # Use reasoning mode`);
        console.log(`  LLM_DEMO_MODE            # Run in demo mode`);
        console.log(`  HTTP_PORT                # Enable web server on port\n`);
        console.log(`${CYAN}v2 Features:${NORMAL}`);
        console.log(`  - Multi-agent orchestration with specialized agents`);
        console.log(`  - Custom data & context management`);
        console.log(`  - Advanced prompt templating system`);
        console.log(`  - Intelligent memory & conversation history`);
        console.log(`  - Task queue & workflow management`);
        console.log(`  - Response caching (24h TTL)`);
        console.log(`  - Configuration management`);
        console.log(`  - Request logging & analytics`);
        console.log(`  - Sentiment analysis`);
        console.log(`  - Profile management\n`);
        process.exit(0);
    } else if (args[0] === 'sentiment' && args.length > 1) {
        const textToAnalyze = args.slice(1).join(' ');
        const start = Date.now();
        const result = analyzeSentiment(textToAnalyze);
        const duration = Date.now() - start;
        console.log(JSON.stringify(result, null, 2));
        logger.logRequest('sentiment', textToAnalyze, JSON.stringify(result), duration);
    } else if (args[0] === 'stats') {
        const stats = logger.getStats();
        if (stats) {
            console.log(JSON.stringify(stats, null, 2));
        } else {
            console.log('No request logs found.');
        }
    } else if (args[0] === 'export' && args.length > 1) {
        const format = args[1].toLowerCase();
        if (format === 'json') {
            console.log(logger.exportJSON());
        } else if (format === 'csv') {
            console.log(logger.exportCSV());
        } else {
            console.error('Unsupported format. Use: json or csv');
            process.exit(-1);
        }
    } else if (args[0] === 'cache-stats') {
        const cacheStats = cache.getStats();
        console.log(JSON.stringify(cacheStats, null, 2));
    } else if (args[0] === 'cache-clear') {
        cache.clear();
        console.log('Cache cleared successfully.');
    } else if (args[0] === 'config-get' && args.length > 1) {
        const value = config.get(args[1]);
        console.log(JSON.stringify(value, null, 2));
    } else if (args[0] === 'config-set' && args.length > 2) {
        try {
            const value = JSON.parse(args[2]);
            config.set(args[1], value);
            console.log(`Configuration updated: ${args[1]} = ${JSON.stringify(value)}`);
        } catch (e) {
            config.set(args[1], args[2]);
            console.log(`Configuration updated: ${args[1]} = ${args[2]}`);
        }
    } else if (args[0] === 'config-list') {
        const profiles = config.listProfiles();
        console.log('Available profiles:');
        if (profiles.length === 0) {
            console.log('  No profiles found.');
        } else {
            profiles.forEach(p => console.log(`  - ${p}`));
        }
    } else if (args[0] === 'agent-list') {
        const allAgents = agents.listAgents();
        console.log(`${CYAN}Available Agents (${allAgents.length}):${NORMAL}\n`);
        allAgents.forEach(agent => {
            console.log(`${BOLD}${agent.name}${NORMAL} (${agent.id})`);
            console.log(`  ${agent.description}`);
            console.log(`  Capabilities: ${agent.capabilities.join(', ')}`);
            if (agent.lastUsedAt) {
                console.log(`  Last used: ${agent.lastUsedAt}`);
            }
            console.log();
        });
    } else if (args[0] === 'agent-activate' && args.length > 1) {
        const agentId = args[1];
        const agent = agents.activateAgent(agentId);
        if (agent) {
            console.log(`${GREEN}${CHECK}${NORMAL} Activated agent: ${BOLD}${agent.name}${NORMAL}`);
            console.log(`System prompt: ${agent.systemPrompt.substring(0, 100)}...`);
        } else {
            console.error(`${RED}Agent '${agentId}' not found${NORMAL}`);
            process.exit(-1);
        }
    } else if (args[0] === 'agent-stats') {
        const stats = agents.getStats();
        console.log('Agent Statistics:');
        stats.forEach(stat => {
            console.log(`\n${stat.name} (${stat.id})`);
            console.log(`  Usage count: ${stat.usageCount}`);
            console.log(`  Total tokens: ${stat.totalTokens}`);
            console.log(`  Created: ${stat.createdAt}`);
            if (stat.lastUsedAt) {
                console.log(`  Last used: ${stat.lastUsedAt}`);
            }
        });
    } else if (args[0] === 'context-create' && args.length > 1) {
        const contextName = args[1];
        const ctx = contextManager.createContext(contextName);
        console.log(`${GREEN}${CHECK}${NORMAL} Created context: ${BOLD}${contextName}${NORMAL}`);
    } else if (args[0] === 'context-list') {
        const contexts = contextManager.listContexts();
        if (contexts.length === 0) {
            console.log('No contexts found.');
        } else {
            console.log(`${CYAN}Contexts (${contexts.length}):${NORMAL}\n`);
            contexts.forEach(ctx => {
                console.log(`${BOLD}${ctx.name}${NORMAL}`);
                console.log(`  Documents: ${ctx.documents}`);
                console.log(`  Tags: ${ctx.tags.join(', ') || 'none'}`);
                console.log(`  Size: ${ctx.size} bytes`);
                console.log(`  Updated: ${ctx.updatedAt}`);
                console.log();
            });
        }
    } else if (args[0] === 'context-activate' && args.length > 1) {
        const contextName = args[1];
        const ctx = contextManager.activateContext(contextName);
        if (ctx) {
            console.log(`${GREEN}${CHECK}${NORMAL} Activated context: ${BOLD}${contextName}${NORMAL}`);
        } else {
            console.error(`${RED}Context '${contextName}' not found${NORMAL}`);
            process.exit(-1);
        }
    } else if (args[0] === 'context-stats') {
        const stats = contextManager.getStats();
        console.log('Context Statistics:');
        console.log(`  Total contexts: ${stats.totalContexts}`);
        console.log(`  Total size: ${stats.totalSize} bytes`);
        console.log(`  Total documents: ${stats.totalDocuments}`);
        console.log(`  Active context: ${stats.activeContext || 'none'}`);
    } else if (args[0] === 'prompt-list') {
        const templates = prompts.listTemplates();
        console.log(`${CYAN}Prompt Templates (${templates.length}):${NORMAL}\n`);
        templates.forEach(template => {
            console.log(`${BOLD}${template.name}${NORMAL} (${template.id})`);
            console.log(`  ${template.description}`);
            console.log(`  Variables: ${template.variables.join(', ')}`);
            console.log(`  Usage: ${template.usageCount} times`);
            console.log();
        });
    } else if (args[0] === 'prompt-render' && args.length > 1) {
        const templateId = args[1];
        const template = prompts.getTemplate(templateId);
        if (template) {
            console.log(`${CYAN}Template: ${template.name}${NORMAL}\n`);
            console.log(template.template);
        } else {
            console.error(`${RED}Template '${templateId}' not found${NORMAL}`);
            process.exit(-1);
        }
    } else if (args[0] === 'prompt-run' && args.length > 1) {
        const templateId = args[1];
        const template = prompts.getTemplate(templateId);
        if (!template) {
            console.error(`${RED}Template '${templateId}' not found${NORMAL}`);
            process.exit(-1);
        }
        const variables = args.slice(2).reduce((acc, pair) => {
            const [key, ...rest] = pair.split('=');
            if (key) {
                acc[key] = rest.join('=');
            }
            return acc;
        }, {});
        const rendered = prompts.render(templateId, variables);
        if (rendered) {
            console.log(rendered);
        } else {
            console.error(`${RED}Failed to render template '${templateId}'${NORMAL}`);
            process.exit(-1);
        }
    } else if (args[0] === 'task-list') {
        const allTasks = tasks.listTasks();
        console.log(`${CYAN}Tasks (${allTasks.length}):${NORMAL}\n`);
        allTasks.forEach(task => {
            const status = task.metadata.status === 'completed' ? GREEN : YELLOW;
            console.log(`${BOLD}${task.name}${NORMAL} [${status}${task.metadata.status}${NORMAL}]`);
            console.log(`  ID: ${task.id}`);
            console.log(`  Type: ${task.type}`);
            console.log(`  Priority: ${task.metadata.priority}`);
        });
    } else if (args[0] === 'task-stats') {
        const stats = tasks.getQueueStats();
        console.log('Task Queue Statistics:');
        console.log(`  Total tasks: ${stats.totalTasks}`);
        console.log(`  Queued: ${stats.queuedCount}`);
        console.log(`  Pending: ${stats.pendingCount}`);
        console.log(`  Running: ${stats.runningCount}`);
        console.log(`  Completed: ${stats.completedCount}`);
        console.log(`  Workflows: ${stats.workflows}`);
    } else if (args[0] === 'memory-list') {
        const conversations = memory.listConversations();
        if (conversations.length === 0) {
            console.log('No conversations found.');
        } else {
            console.log(`${CYAN}Conversations (${conversations.length}):${NORMAL}\n`);
            conversations.forEach(conv => {
                console.log(`${BOLD}${conv.id}${NORMAL}`);
                console.log(`  Messages: ${conv.messageCount}`);
                console.log(`  Tokens: ${conv.tokenCount}`);
                console.log(`  Created: ${conv.createdAt}`);
                console.log(`  Topics: ${conv.topics.join(', ') || 'none'}`);
                console.log();
            });
        }
    } else if (args[0] === 'memory-stats') {
        const stats = memory.getStats();
        console.log('Memory Statistics:');
        console.log(`  Total conversations: ${stats.totalConversations}`);
        console.log(`  Total messages: ${stats.totalMessages}`);
        console.log(`  Total tokens: ${stats.totalTokens}`);
        console.log(`  Short-term memory: ${stats.shortTermMemorySize} items`);
        console.log(`  Long-term memory: ${stats.longTermMemorySize} items`);
    } else {
        await canary(); // Only run canary if not directly testing sentiment
        if (args.length > 0) {
            args.forEach(evaluate);
        } else {
            const port = parseInt(process.env.HTTP_PORT, 10);
            if (!Number.isNaN(port) && port > 0 && port < 65536) {
                await serve(port);
            } else {
                await interact();
            }
        }
    }
})();
