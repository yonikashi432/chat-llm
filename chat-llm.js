#!/usr/bin/env -S sh -c 'command -v node >/dev/null && exec node "$0" "$@" || exec bun "$0" "$@" '

const fs = require('fs');
const http = require('http');
const readline = require('readline');
const { analyzeSentiment } = require('./tools/sentiment_analyzer');
const { RequestLogger } = require('./tools/request-logger');
const { ResponseCache } = require('./tools/response-cache');
const { ConfigManager } = require('./tools/config-manager');
const { AgentManager } = require('./tools/agent-manager');
const { ContextManager } = require('./tools/context-manager');
const { PromptManager } = require('./tools/prompt-manager');
const { MemoryManager } = require('./tools/memory-manager');
const { TaskManager } = require('./tools/task-manager');

const LLM_API_BASE_URL = process.env.LLM_API_BASE_URL || 'https://api.openai.com/v1';
const LLM_API_KEY = process.env.LLM_API_KEY || process.env.OPENAI_API_KEY;
const LLM_CHAT_MODEL = process.env.LLM_CHAT_MODEL;
const LLM_STREAMING = process.env.LLM_STREAMING !== 'no';
const LLM_FORCE_REASONING = process.env.LLM_FORCE_REASONING;

let LLM_MODEL_CONFIGS = {};
if (process.env.LLM_MODEL_CONFIGS) {
    try {
        LLM_MODEL_CONFIGS = JSON.parse(process.env.LLM_MODEL_CONFIGS);
    } catch (e) {
        console.error(`${RED}Error parsing LLM_MODEL_CONFIGS: ${e.message}${NORMAL}`);
        process.exit(-1);
    }
}

const LLM_DEBUG = process.env.LLM_DEBUG;
const LLM_DEBUG_FAIL_EXIT = process.env.LLM_DEBUG_FAIL_EXIT;

const NORMAL = '\x1b[0m';
const BOLD = '\x1b[1m';
const YELLOW = '\x1b[93m';
const MAGENTA = '\x1b[35m';
const RED = '\x1b[91m';
const GREEN = '\x1b[92m';
const CYAN = '\x1b[36m';
const GRAY = '\x1b[90m';
const ARROW = '⇢';
const CHECK = '✓';
const CROSS = '✘';

// Initialize v2 managers
const cache = new ResponseCache('./cache');
const config = new ConfigManager('./config');
const logger = new RequestLogger('./logs');
const agents = new AgentManager();
const context = new ContextManager('./context-data');
const prompts = new PromptManager();
const memory = new MemoryManager('./memory');
const tasks = new TaskManager();

/**
 * Suspends the execution for a specified amount of time.
 *
 * @param {number} ms - The amount of time to suspend execution in milliseconds.
 * @returns {Promise<void>} - A promise that resolves after the specified time has elapsed.
 */
const sleep = async (ms) => new Promise((resolve) => setTimeout(resolve, ms));

const MAX_RETRY_ATTEMPT = 3;


/**
 * Represents a chat message.
 *
 * @typedef {Object} Message
 * @property {'system'|'user'|'assistant'} role
 * @property {string} content
 */

/**
 * A callback function to stream then completion.
 *
 * @callback CompletionHandler
 * @param {string} text
 * @returns {void}
 */

/**
 * Generates a chat completion using a RESTful LLM API service.
 *
 * @param {Array<Message>} messages - List of chat messages.
 * @param {CompletionHandler=} handler - An optional callback to stream the completion.
 * @returns {Promise<string>} The completion generated by the LLM.
 */

const chat = async (messages, handler = null, attempt = MAX_RETRY_ATTEMPT, modelName = null) => {
    const timeout = 17; // seconds
    const stream = LLM_STREAMING && typeof handler === 'function';

    let apiBaseUrl = LLM_API_BASE_URL;
    let apiKey = LLM_API_KEY;
    let chatModel = LLM_CHAT_MODEL || 'gpt-5-nano';

    if (modelName && LLM_MODEL_CONFIGS[modelName]) {
        const config = LLM_MODEL_CONFIGS[modelName];
        apiBaseUrl = config.api_base_url || apiBaseUrl;
        apiKey = config.api_key || apiKey;
        chatModel = config.chat_model || chatModel;
    }

    const url = `${apiBaseUrl}/chat/completions`
    const auth = (apiKey) ? { 'Authorization': `Bearer ${apiKey}` } : {};
    const stop = ['\\boxed', '</s>', '</s>', '</s>'];

    const body = { messages, model: chatModel, stop, stream }

    LLM_DEBUG &&
        messages.forEach(({ role, content }) => {
            console.log(`${MAGENTA}${role}:${NORMAL} ${content}`);
        });

    try {

        const response = await fetch(url, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json', ...auth },
            body: JSON.stringify(body),
            signal: AbortSignal.timeout(timeout * 1000)
        });
        if (!response.ok) {
            const status = response.status;
            const statusText = response.statusText;
            
            // Handle rate limiting with exponential backoff
            if (status === 429 && attempt > 1) {
                const waitTime = (MAX_RETRY_ATTEMPT - attempt + 2) * 5000; // 5s, 10s, 15s
                LLM_DEBUG && console.log(`Rate limited (429). Waiting ${waitTime}ms before retry...`);
                await sleep(waitTime);
                return await chat(messages, handler, attempt - 1);
            }
            
            throw new Error(`HTTP error with the status: ${status} ${statusText}`);
        }

        const extract = (data) => {
            const { choices, candidates } = data;
            const first = choices ? choices[0] : candidates[0];
            if (first?.content || first?.message) {
                const content = first?.content ? first.content : first.message.content;
                const parts = content?.parts;
                const answer = parts ? parts.map(part => part.text).join('') : content;
                return answer;
            }
            return '';
        }

        if (!stream) {
            const data = await response.json();
            const answer = extract(data).trim();
            if (LLM_DEBUG) {
                console.log(`${YELLOW}${answer}${NORMAL}`);
            }
            (answer.length > 0) && handler && handler(answer);
            return answer;
        }

        const parse = (line) => {
            const separator = line.indexOf(':');
            if (separator < 0) {
                return '';
            }
            const key = line.substring(0, separator).trim();
            const payload = line.substring(separator + 1);
            if (key === 'data') {
                let partial = null;
                try {
                    const { choices } = JSON.parse(payload);
                    const [choice] = choices;
                    const { delta } = choice;
                    partial = delta?.content || '';
                } catch (e) {
                    // ignore
                } finally {
                    return partial;
                }
            } else {
                return '';
            }
        }

        const reader = response.body.getReader();
        const decoder = new TextDecoder();

        let answer = '';
        let buffer = '';
        while (true) {
            const { value, done } = await reader.read();
            if (done) {
                break;
            }
            const lines = decoder.decode(value).split('\n');
            for (let i = 0; i < lines.length; ++i) {
                const line = buffer + lines[i];
                if (line[0] === ':') {
                    buffer = '';
                    continue;
                }
                if (line === 'data: [DONE]') {
                    break;
                }
                if (line.length > 0) {
                    const partial = parse(line.trim());
                    if (partial === null) {
                        buffer = line;
                    } else if (partial && partial.length > 0) {
                        buffer = '';
                        if (answer.length < 1) {
                            const leading = partial.trim();
                            answer = leading;
                            handler && (leading.length > 0) && handler(leading);
                        } else {
                            answer += partial;
                            handler && handler(partial);
                        }
                    }
                }
            }
        }
        return answer;
    } catch (e) {
        if (e.name === 'TimeoutError') {
            LLM_DEBUG && console.log(`Timeout with LLM chat after ${timeout} seconds`);
        }
        if (attempt > 1 && (e.name === 'TimeoutError' || e.name === 'EvalError')) {
            LLM_DEBUG && console.log('Retrying...');
            await sleep((MAX_RETRY_ATTEMPT - attempt + 1) * 1500);
            return await chat(messages, handler, attempt - 1);
        } else {
            throw e;
        }
    }
}

const REPLY_PROMPT = `You are a helpful AI assistant. You are chatting with a human user.
Your answer should be a sentence or two, unless the user's request requires long-form outputs.
Never use emojis. Never use markdown. Always answer in plain text.`;

const REPLY_THINK = `You are a helpful AI assistant. You are chatting with a human user.

You have access to the following tools:
- analyzeSentiment(text: string): Analyzes the sentiment of the given text. Returns an object with 'sentiment' (positive, negative, neutral) and 'score'.

You can also select a specific LLM model to use for your response.
Available models are: ${Object.keys(LLM_MODEL_CONFIGS).length > 0 ? Object.keys(LLM_MODEL_CONFIGS).join(', ') : 'None configured, using default.'}
To use a specific model, include <use_model>model_name</use_model> in your prompt.

You should first draft your thinking process (inner monologue) until you have derived the final answer.
Write both your thoughts and answer in the same language as the task posed by the user.

Your thinking process must follow the template below:

<think>
Your thoughts or/and draft, like working through an exercise on scratch paper.
Be as casual and as long as you want until you are confident to generate a correct answer.
If you need to use a tool, use the following format:
<tool_code>
console.log(analyzeSentiment("text to analyze"));
</tool_code>
</think>

Your answer should be a sentence or two, unless the user's request requires long-form outputs.
Never use emojis. Never use markdown. Always answer in plain text and  in the same language as the query.`;

const DEMO_RESPONSES = {
    'capital': ['Paris is the capital of France.', 'The capital of France is Paris.'],
    'weather': ['I cannot check the current weather, but you can check a weather service online.', 'Weather information is not available in demo mode.'],
    'time': ['I cannot provide real-time information in demo mode.', 'Time-based queries are not supported in demo mode.'],
    'default': ['This is a demo response since the LLM API is unavailable.', 'I am in demo mode and cannot provide real responses.', 'Demo mode is active - API responses are simulated.']
};

/**
 * Generates a simulated demo response for testing without API access.
 * Uses simple pattern matching to provide contextually appropriate responses.
 * 
 * @param {string} inquiry - The user's question or request
 * @returns {Promise<string>} A simulated response based on inquiry patterns
 */
const demoReply = async (inquiry) => {
    // Simple pattern matching for demo responses
    const lowerInquiry = inquiry.toLowerCase();
    
    if (lowerInquiry.includes('capital') || lowerInquiry.includes('france') || lowerInquiry.includes('paris')) {
        return DEMO_RESPONSES.capital[Math.floor(Math.random() * DEMO_RESPONSES.capital.length)];
    } else if (lowerInquiry.includes('weather') || lowerInquiry.includes('rain') || lowerInquiry.includes('temperature')) {
        return DEMO_RESPONSES.weather[Math.floor(Math.random() * DEMO_RESPONSES.weather.length)];
    } else if (lowerInquiry.includes('time') || lowerInquiry.includes('hour') || lowerInquiry.includes('clock')) {
        return DEMO_RESPONSES.time[Math.floor(Math.random() * DEMO_RESPONSES.time.length)];
    }
    
    return DEMO_RESPONSES.default[Math.floor(Math.random() * DEMO_RESPONSES.default.length)];
};

/**
 * Generates a reply to user inquiry using the LLM API with caching and history support.
 * 
 * @param {Object} context - Conversation context
 * @param {string} context.inquiry - User's current question or request
 * @param {Array<Object>} context.history - Previous conversation messages
 * @param {Object} context.delegates - Optional callback handlers
 * @param {Function} context.delegates.stream - Optional streaming handler for real-time responses
 * @returns {Promise<Object>} Response object containing the answer and updated context
 */
const reply = async (context) => {
    const { inquiry, history, delegates } = context;
    const { stream } = delegates || {};

    // Check cache first if caching is enabled
    let cacheEnabled = config.get('caching.enabled', true);
    let cachedResponse = cacheEnabled ? cache.get(inquiry) : null;
    
    if (cachedResponse) {
        if (typeof stream === 'function') {
            stream(cachedResponse);
        }
        logger.logRequest('reply', inquiry, cachedResponse, 0, { cached: true });
        return { answer: cachedResponse, ...context };
    }

    const messages = [];
    messages.push({ role: 'system', content: LLM_FORCE_REASONING ? REPLY_THINK : REPLY_PROMPT });
    const relevant = history.slice(-5);
    relevant.forEach(msg => {
        const { inquiry, answer } = msg;
        messages.push({ role: 'user', content: inquiry });
        messages.push({ role: 'assistant', content: answer });
    });

    messages.push({ role: 'user', content: inquiry });

    let selectedModel = null;
    // Check if the user explicitly requested a model in the inquiry
    const modelMatch = inquiry.match(/<use_model>(.*?)<\/use_model>/);
    if (modelMatch && modelMatch[1]) {
        selectedModel = modelMatch[1].trim();
    }
    
    let rawAnswer;
    try {
        const start = Date.now();
        rawAnswer = await chat(messages, stream, MAX_RETRY_ATTEMPT, selectedModel);
        const duration = Date.now() - start;
        logger.logRequest('reply', inquiry, rawAnswer.substring(0, 100), duration);
    } catch (error) {
        if (process.env.LLM_DEMO_MODE) {
            rawAnswer = await demoReply(inquiry);
        } else {
            throw error;
        }
    }

    const THINK_START_TAG = '<think>';
    const THINK_STOP_TAG = '</think>';
    const TOOL_CODE_START_TAG = '<tool_code>';
    const TOOL_CODE_STOP_TAG = '</tool_code>';

    const thinkStartIndex = rawAnswer.indexOf(THINK_START_TAG);
    const thinkStopIndex = rawAnswer.indexOf(THINK_STOP_TAG);


    if (thinkStartIndex !== -1 && thinkStopIndex !== -1) {
        const thinkContent = rawAnswer.substring(thinkStartIndex + THINK_START_TAG.length, thinkStopIndex);
        const toolCodeStartIndex = thinkContent.indexOf(TOOL_CODE_START_TAG);
        const toolCodeStopIndex = thinkContent.indexOf(TOOL_CODE_STOP_TAG);

        if (toolCodeStartIndex !== -1 && toolCodeStopIndex !== -1) {
            const toolCode = thinkContent.substring(toolCodeStartIndex + TOOL_CODE_START_TAG.length, toolCodeStopIndex);
            let toolOutput = '';
            try {
                // Execute the tool code in a sandboxed environment
                // For simplicity, using eval here. In a real-world scenario, consider a more secure sandbox.
                const consoleLog = (output) => { toolOutput += JSON.stringify(output) + '\n'; };
                const context = { analyzeSentiment, console: { log: consoleLog } };
                const script = new Function('analyzeSentiment', 'console', toolCode);
                script(context.analyzeSentiment, context.console);
            } catch (e) {
                toolOutput = `Error executing tool: ${e.message}`;
            }

            // Re-prompt the LLM with the tool output
            messages.push({ role: 'assistant', content: rawAnswer }); // Add the LLM's initial thought process
            messages.push({ role: 'system', content: `Tool output:\n${toolOutput}` });
            messages.push({ role: 'user', content: 'Given the tool output, provide your final answer.' });
            rawAnswer = await chat(messages, stream);
        }
    }

    if (cacheEnabled && rawAnswer && rawAnswer.length > 0) {
        cache.set(inquiry, rawAnswer);
    }

    return { answer: rawAnswer, ...context };
}

/**
 * Converts an expected answer into a suitable regular expression array.
 *
 * @param {string} match
 * @returns {Array<RegExp>}
 */
const regexify = (match) => {
    const filler = (text, index) => {
        let i = index;
        while (i < text.length) {
            if (text[i] === '/') {
                break;
            }
            ++i;
        }
        return i;
    };

    const pattern = (text, index) => {
        let i = index;
        if (text[i] === '/') {
            ++i;
            while (i < text.length) {
                if (text[i] === '/' && text[i - 1] !== '\\') {
                    break;
                }
                ++i;
            }
        }
        return i;
    };

    const regexes = [];
    let pos = 0;
    while (pos < match.length) {
        pos = filler(match, pos);
        const next = pattern(match, pos);
        if (next > pos && next < match.length) {
            const sub = match.substring(pos + 1, next);
            const regex = RegExp(sub, 'gi');
            regexes.push(regex);
            pos = next + 1;
        } else {
            break;
        }
    }

    if (regexes.length === 0) {
        regexes.push(RegExp(match, 'gi'));
    }

    return regexes;
}

/**
 * Returns all possible matches given a list of regular expressions.
 *
 * @param {string} text
 * @param {Array<RegExp>} regexes
 * @returns {Array<Span>}
 */
const match = (text, regexes) => {
    return regexes.map(regex => {
        const match = regex.exec(text);
        if (!match) {
            return null;
        }
        const [first] = match;
        const { index } = match;
        const { length } = first;
        return { index, length };
    }).filter(span => span !== null);
}

/**
 * Formats the input (using ANSI colors) to highlight the spans.
 *
 * @param {string} text
 * @param {Array<Span>} spans
 * @param {string} color
 * @returns {string}
 */

const highlight = (text, spans, color = BOLD + GREEN) => {
    let result = text;
    spans.sort((p, q) => q.index - p.index).forEach((span) => {
        const { index, length } = span;
        const prefix = result.substring(0, index);
        const content = result.substring(index, index + length);
        const suffix = result.substring(index + length);
        result = `${prefix}${color}${content}${NORMAL}${suffix}`;
    });
    return result;
}

/**
 * Evaluates a test file and executes the test cases.
 *
 * @param {string} filename - The path to the test file.
 */
const evaluate = async (filename) => {
    try {
        let history = [];
        let total = 0;
        let failures = 0;

        const handle = async (line) => {
            const parts = (line && line.length > 0) ? line.split(':') : [];
            if (parts.length >= 2) {
                const role = parts[0];
                const content = line.slice(role.length + 1).trim();
                if (role === 'Story') {
                    console.log();
                    console.log('-----------------------------------');
                    console.log(`Story: ${MAGENTA}${BOLD}${content}${NORMAL}`);
                    console.log('-----------------------------------');
                    history = [];
                } else if (role === 'User') {
                    const inquiry = content;
                    const context = { inquiry, history };
                    process.stdout.write(`  ${inquiry}\r`);
                    const start = Date.now();
                    const result = await reply(context);
                    const duration = Date.now() - start;
                    const { answer } = result;
                    history.push({ inquiry, answer: unthink(answer).trim(), duration });
                    ++total;
                } else if (role === 'Assistant') {
                    const expected = content;
                    const last = history.slice(-1).pop();
                    if (!last) {
                        console.error('There is no answer yet!');
                        process.exit(-1);
                    } else {
                        const { inquiry, answer, duration } = last;
                        const target = answer;
                        const regexes = regexify(expected);
                        const matches = match(target, regexes);
                        if (matches.length === regexes.length) {
                            console.log(`${GREEN}${CHECK} ${CYAN}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.log(' ', highlight(target, matches));
                        } else {
                            ++failures;
                            console.error(`${RED}${CROSS} ${YELLOW}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.error(`Expected ${role} to contain: ${CYAN}${regexes.join(',')}${NORMAL}`);
                            console.error(`Actual ${role}: ${MAGENTA}${target}${NORMAL}`);
                            LLM_DEBUG_FAIL_EXIT && process.exit(-1);
                        }
                    }
                }
            }
        };

        const trim = (input) => {
            const text = input.trim();
            const marker = text.indexOf('#');
            if (marker >= 0) {
                return text.substr(0, marker).trim();
            }
            return text;
        }

        const lines = fs.readFileSync(filename, 'utf-8').split('\n').map(trim);
        for (const i in lines) {
            await handle(lines[i]);
        }
        if (failures <= 0) {
            console.log(`${GREEN}${CHECK}${NORMAL} SUCCESS: ${GREEN}${total} test(s)${NORMAL}.`);
        } else {
            console.log(`${RED}${CROSS}${NORMAL} FAIL: ${GRAY}${total} test(s), ${RED}${failures} failure(s)${NORMAL}.`);
            process.exit(-1);
        }
    } catch (e) {
        console.error('ERROR:', e.toString());
        process.exit(-1);
    }
}

const THINK_START = '<think>';
const THINK_STOP = '</think>';

const unthink = (input) => {
    const start = input.indexOf(THINK_START);
    if (start < 0) {
        return input;
    }
    const end = input.indexOf(THINK_STOP);
    if (end < 0) {
        return input.substring(0, start);
    }
    return input.substring(0, start) + input.substring(end + 8);
};

const push = (display, input, threshold = THINK_START.length) => {
    let { buffer, written, print } = display;
    buffer += input;
    if (buffer.length < threshold) {
        return { buffer, written: '', print };
    }
    const incoming = unthink(buffer).trim();
    if (incoming.length > written.length) {
        const delta = incoming.substring(written.length);
        print && print(delta);
        written = incoming;
    }
    return { buffer, written, print };
};

const flush = (display) => push(display, '', 0);

/**
 * Represents the contextual information for each pipeline stage.
 *
 * @typedef {Object} Context
 * @property {Array<object>} history
 * @property {string} inquiry
 * @property {string} answer
 * @property {Object.<string, function>} delegates - Impure functions to access the outside world.
 */

const interact = async () => {
    const history = [];

    let loop = true;
    const io = readline.createInterface({ input: process.stdin, output: process.stdout });
    io.on('close', () => { loop = false; });

    const qa = () => {
        io.question(`${YELLOW}>> ${CYAN}`, async (inquiry) => {
            process.stdout.write(NORMAL);
            const print = (text) => process.stdout.write(text);
            let display = { buffer: '', written: '', print };
            const stream = (text) => display = push(display, text);
            const delegates = { stream };
            const context = { inquiry, history, delegates };
            const start = Date.now();
            await reply(context);
            const duration = Date.now() - start;
            display = flush(display);
            const answer = display.written;
            history.push({ inquiry, answer, duration });
            console.log();
            console.log();
            loop && qa();
        })
    }

    qa();
}

/**
 * Starts an HTTP server that listens on the specified port and serves requests.
 *
 * @param {number} port - The port number to listen on.
 */
const serve = async (port) => {
    let history = [];

    const decode = (url) => {
        const parsedUrl = new URL(`http://localhost/${url}`);
        const { search } = parsedUrl;
        return decodeURIComponent(search.substring(1)).trim();
    };

    const server = http.createServer(async (request, response) => {
        const { url } = request;
        if (url === '/health') {
            response.writeHead(200).end('OK');
        } else if (url === '/' || url === '/index.html') {
            response.writeHead(200, { 'Content-Type': 'text/html' });
            response.end(fs.readFileSync('./index.html'));
        } else if (url.startsWith('/chat')) {
            const inquiry = decode(url);
            if (inquiry === '/reset') {
                history = [];
                response.write('History cleared.');
                response.end();
            } else if (inquiry.length > 0) {
                console.log(`${YELLOW}>> ${CYAN}${inquiry}${NORMAL}`);
                response.writeHead(200, { 'Content-Type': 'text/plain' });

                const stream = (text) => {
                    process.stdout.write(text);
                    response.write(text);
                }
                const delegates = { stream };
                const context = { inquiry, history, delegates };
                const start = Date.now();
                const { answer } = await reply(context);
                console.log();
                response.end();
                const duration = Date.now() - start;
                history.push({ inquiry, answer, duration });
            } else {
                response.writeHead(400).end();
            }
        } else {
            console.error(`${url} is 404!`);
            response.writeHead(404);
            response.end();
        }
    });
    server.listen(port);
    console.log('Listening on port', port);
};

const canary = async () => {
    console.log(`Using LLM at ${YELLOW}${LLM_API_BASE_URL}${NORMAL} (model: ${GREEN}${LLM_CHAT_MODEL || 'default'}${NORMAL}).`);
    process.stdout.write(`${ARROW} Checking LLM...\r`);

    const inquiry = 'What is the capital of France?';
    const history = [];
    const context = { inquiry, history };
    try {
        const { answer } = await reply(context);
        LLM_REASONING_ABILITY = answer.includes(THINK_START) && answer.includes(THINK_STOP);
        console.log(`LLM is ${GREEN}ready${NORMAL} (working as expected).`);
        if (LLM_REASONING_ABILITY) {
            console.log(`This is a ${YELLOW}reasoning${NORMAL} model.`);
        } else {
            console.log(`This is a regular model, ${MAGENTA}not${NORMAL} capable of self-reasoning.`);
        }
        console.log();
    } catch (error) {
        if (process.env.LLM_DEMO_MODE) {
            console.log(`${YELLOW}⚠${NORMAL}  LLM unavailable - running in ${CYAN}demo mode${NORMAL}.`);
            console.log('   (Responses will be simulated for testing purposes)\n');
            return;
        }
        console.error(`${CROSS} ${RED}Fatal error: LLM is not ready!${NORMAL}`);
        console.error(error);
        process.exit(-1);
    }
};

(async () => {
    const args = process.argv.slice(2);
    
    if (args.length === 0 || args[0] === '--help' || args[0] === '-h' || args[0] === 'help') {
        console.log(`${BOLD}Chat LLM v2${NORMAL} - Swiss Army Knife LLM Agent\n`);
        console.log(`${CYAN}Usage:${NORMAL}`);
        console.log(`  ./chat-llm.js                              # Interactive mode`);
        console.log(`  ./chat-llm.js <test-file>                 # Run test file`);
        console.log(`  HTTP_PORT=5000 ./chat-llm.js              # Web interface\n`);
        console.log(`${CYAN}Agent & Delegation:${NORMAL}`);
        console.log(`  ./chat-llm.js agent-list                  # List available agents`);
        console.log(`  ./chat-llm.js agent-activate <id>         # Activate an agent`);
        console.log(`  ./chat-llm.js agent-stats                 # Agent usage statistics\n`);
        console.log(`${CYAN}Context Management:${NORMAL}`);
        console.log(`  ./chat-llm.js context-create <name>       # Create new context`);
        console.log(`  ./chat-llm.js context-list                # List all contexts`);
        console.log(`  ./chat-llm.js context-activate <name>     # Activate context`);
        console.log(`  ./chat-llm.js context-stats               # Context statistics\n`);
        console.log(`${CYAN}Prompt Management:${NORMAL}`);
        console.log(`  ./chat-llm.js prompt-list                 # List all templates`);
        console.log(`  ./chat-llm.js prompt-render <id>          # Render template\n`);
        console.log(`${CYAN}Task Management:${NORMAL}`);
        console.log(`  ./chat-llm.js task-list                   # List all tasks`);
        console.log(`  ./chat-llm.js task-stats                  # Task queue statistics\n`);
        console.log(`${CYAN}Memory & History:${NORMAL}`);
        console.log(`  ./chat-llm.js memory-list                 # List conversations`);
        console.log(`  ./chat-llm.js memory-stats                # Memory statistics\n`);
        console.log(`${CYAN}Analysis & Logging:${NORMAL}`);
        console.log(`  ./chat-llm.js sentiment <text>            # Analyze sentiment`);
        console.log(`  ./chat-llm.js stats                       # Show request statistics`);
        console.log(`  ./chat-llm.js export <format>             # Export logs (json|csv)\n`);
        console.log(`${CYAN}Cache & Configuration:${NORMAL}`);
        console.log(`  ./chat-llm.js cache-stats                 # Show cache statistics`);
        console.log(`  ./chat-llm.js cache-clear                 # Clear response cache`);
        console.log(`  ./chat-llm.js config-get <key>            # Get config value`);
        console.log(`  ./chat-llm.js config-set <key> <value>    # Set config value`);
        console.log(`  ./chat-llm.js config-list                 # List all profiles\n`);
        console.log(`${CYAN}Environment Variables:${NORMAL}`);
        console.log(`  LLM_API_BASE_URL         # API endpoint (default: OpenAI)`);
        console.log(`  LLM_API_KEY              # API authentication key`);
        console.log(`  LLM_CHAT_MODEL           # Model name to use`);
        console.log(`  LLM_STREAMING            # Enable streaming (default: yes)`);
        console.log(`  LLM_FORCE_REASONING      # Use reasoning mode`);
        console.log(`  LLM_DEMO_MODE            # Run in demo mode`);
        console.log(`  HTTP_PORT                # Enable web server on port\n`);
        console.log(`${CYAN}v2 Features:${NORMAL}`);
        console.log(`  - Multi-agent orchestration with specialized agents`);
        console.log(`  - Custom data & context management`);
        console.log(`  - Advanced prompt templating system`);
        console.log(`  - Intelligent memory & conversation history`);
        console.log(`  - Task queue & workflow management`);
        console.log(`  - Response caching (24h TTL)`);
        console.log(`  - Configuration management`);
        console.log(`  - Request logging & analytics`);
        console.log(`  - Sentiment analysis`);
        console.log(`  - Profile management\n`);
        process.exit(0);
    } else if (args[0] === 'sentiment' && args.length > 1) {
        const textToAnalyze = args.slice(1).join(' ');
        const start = Date.now();
        const result = analyzeSentiment(textToAnalyze);
        const duration = Date.now() - start;
        console.log(JSON.stringify(result, null, 2));
        logger.logRequest('sentiment', textToAnalyze, JSON.stringify(result), duration);
    } else if (args[0] === 'stats') {
        const stats = logger.getStats();
        if (stats) {
            console.log(JSON.stringify(stats, null, 2));
        } else {
            console.log('No request logs found.');
        }
    } else if (args[0] === 'export' && args.length > 1) {
        const format = args[1].toLowerCase();
        if (format === 'json') {
            console.log(logger.exportJSON());
        } else if (format === 'csv') {
            console.log(logger.exportCSV());
        } else {
            console.error('Unsupported format. Use: json or csv');
            process.exit(-1);
        }
    } else if (args[0] === 'cache-stats') {
        const cacheStats = cache.getStats();
        console.log(JSON.stringify(cacheStats, null, 2));
    } else if (args[0] === 'cache-clear') {
        cache.clear();
        console.log('Cache cleared successfully.');
    } else if (args[0] === 'config-get' && args.length > 1) {
        const value = config.get(args[1]);
        console.log(JSON.stringify(value, null, 2));
    } else if (args[0] === 'config-set' && args.length > 2) {
        try {
            const value = JSON.parse(args[2]);
            config.set(args[1], value);
            console.log(`Configuration updated: ${args[1]} = ${JSON.stringify(value)}`);
        } catch (e) {
            config.set(args[1], args[2]);
            console.log(`Configuration updated: ${args[1]} = ${args[2]}`);
        }
    } else if (args[0] === 'config-list') {
        const profiles = config.listProfiles();
        console.log('Available profiles:');
        if (profiles.length === 0) {
            console.log('  No profiles found.');
        } else {
            profiles.forEach(p => console.log(`  - ${p}`));
        }
    } else if (args[0] === 'agent-list') {
        const allAgents = agents.listAgents();
        console.log(`${CYAN}Available Agents (${allAgents.length}):${NORMAL}\n`);
        allAgents.forEach(agent => {
            console.log(`${BOLD}${agent.name}${NORMAL} (${agent.id})`);
            console.log(`  ${agent.description}`);
            console.log(`  Capabilities: ${agent.capabilities.join(', ')}`);
            if (agent.lastUsedAt) {
                console.log(`  Last used: ${agent.lastUsedAt}`);
            }
            console.log();
        });
    } else if (args[0] === 'agent-activate' && args.length > 1) {
        const agentId = args[1];
        const agent = agents.activateAgent(agentId);
        if (agent) {
            console.log(`${GREEN}${CHECK}${NORMAL} Activated agent: ${BOLD}${agent.name}${NORMAL}`);
            console.log(`System prompt: ${agent.systemPrompt.substring(0, 100)}...`);
        } else {
            console.error(`${RED}Agent '${agentId}' not found${NORMAL}`);
            process.exit(-1);
        }
    } else if (args[0] === 'agent-stats') {
        const stats = agents.getStats();
        console.log('Agent Statistics:');
        stats.forEach(stat => {
            console.log(`\n${stat.name} (${stat.id})`);
            console.log(`  Usage count: ${stat.usageCount}`);
            console.log(`  Total tokens: ${stat.totalTokens}`);
            console.log(`  Created: ${stat.createdAt}`);
            if (stat.lastUsedAt) {
                console.log(`  Last used: ${stat.lastUsedAt}`);
            }
        });
    } else if (args[0] === 'context-create' && args.length > 1) {
        const contextName = args[1];
        const ctx = context.createContext(contextName);
        console.log(`${GREEN}${CHECK}${NORMAL} Created context: ${BOLD}${contextName}${NORMAL}`);
    } else if (args[0] === 'context-list') {
        const contexts = context.listContexts();
        if (contexts.length === 0) {
            console.log('No contexts found.');
        } else {
            console.log(`${CYAN}Contexts (${contexts.length}):${NORMAL}\n`);
            contexts.forEach(ctx => {
                console.log(`${BOLD}${ctx.name}${NORMAL}`);
                console.log(`  Documents: ${ctx.documents}`);
                console.log(`  Tags: ${ctx.tags.join(', ') || 'none'}`);
                console.log(`  Size: ${ctx.size} bytes`);
                console.log(`  Updated: ${ctx.updatedAt}`);
                console.log();
            });
        }
    } else if (args[0] === 'context-activate' && args.length > 1) {
        const contextName = args[1];
        const ctx = context.activateContext(contextName);
        if (ctx) {
            console.log(`${GREEN}${CHECK}${NORMAL} Activated context: ${BOLD}${contextName}${NORMAL}`);
        } else {
            console.error(`${RED}Context '${contextName}' not found${NORMAL}`);
            process.exit(-1);
        }
    } else if (args[0] === 'context-stats') {
        const stats = context.getStats();
        console.log('Context Statistics:');
        console.log(`  Total contexts: ${stats.totalContexts}`);
        console.log(`  Total size: ${stats.totalSize} bytes`);
        console.log(`  Total documents: ${stats.totalDocuments}`);
        console.log(`  Active context: ${stats.activeContext || 'none'}`);
    } else if (args[0] === 'prompt-list') {
        const templates = prompts.listTemplates();
        console.log(`${CYAN}Prompt Templates (${templates.length}):${NORMAL}\n`);
        templates.forEach(template => {
            console.log(`${BOLD}${template.name}${NORMAL} (${template.id})`);
            console.log(`  ${template.description}`);
            console.log(`  Variables: ${template.variables.join(', ')}`);
            console.log(`  Usage: ${template.usageCount} times`);
            console.log();
        });
    } else if (args[0] === 'prompt-render' && args.length > 1) {
        const templateId = args[1];
        const template = prompts.getTemplate(templateId);
        if (template) {
            console.log(`${CYAN}Template: ${template.name}${NORMAL}\n`);
            console.log(template.template);
        } else {
            console.error(`${RED}Template '${templateId}' not found${NORMAL}`);
            process.exit(-1);
        }
    } else if (args[0] === 'task-list') {
        const allTasks = tasks.listTasks();
        console.log(`${CYAN}Tasks (${allTasks.length}):${NORMAL}\n`);
        allTasks.forEach(task => {
            const status = task.metadata.status === 'completed' ? GREEN : YELLOW;
            console.log(`${BOLD}${task.name}${NORMAL} [${status}${task.metadata.status}${NORMAL}]`);
            console.log(`  ID: ${task.id}`);
            console.log(`  Type: ${task.type}`);
            console.log(`  Priority: ${task.metadata.priority}`);
        });
    } else if (args[0] === 'task-stats') {
        const stats = tasks.getQueueStats();
        console.log('Task Queue Statistics:');
        console.log(`  Total tasks: ${stats.totalTasks}`);
        console.log(`  Queued: ${stats.queuedCount}`);
        console.log(`  Pending: ${stats.pendingCount}`);
        console.log(`  Running: ${stats.runningCount}`);
        console.log(`  Completed: ${stats.completedCount}`);
        console.log(`  Workflows: ${stats.workflows}`);
    } else if (args[0] === 'memory-list') {
        const conversations = memory.listConversations();
        if (conversations.length === 0) {
            console.log('No conversations found.');
        } else {
            console.log(`${CYAN}Conversations (${conversations.length}):${NORMAL}\n`);
            conversations.forEach(conv => {
                console.log(`${BOLD}${conv.id}${NORMAL}`);
                console.log(`  Messages: ${conv.messageCount}`);
                console.log(`  Tokens: ${conv.tokenCount}`);
                console.log(`  Created: ${conv.createdAt}`);
                console.log(`  Topics: ${conv.topics.join(', ') || 'none'}`);
                console.log();
            });
        }
    } else if (args[0] === 'memory-stats') {
        const stats = memory.getStats();
        console.log('Memory Statistics:');
        console.log(`  Total conversations: ${stats.totalConversations}`);
        console.log(`  Total messages: ${stats.totalMessages}`);
        console.log(`  Total tokens: ${stats.totalTokens}`);
        console.log(`  Short-term memory: ${stats.shortTermMemorySize} items`);
        console.log(`  Long-term memory: ${stats.longTermMemorySize} items`);
    } else {
        await canary(); // Only run canary if not directly testing sentiment
        if (args.length > 0) {
            args.forEach(evaluate);
        } else {
            const port = parseInt(process.env.HTTP_PORT, 10);
            if (!Number.isNaN(port) && port > 0 && port < 65536) {
                await serve(port);
            } else {
                await interact();
            }
        }
    }
})();
