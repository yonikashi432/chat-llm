#!/usr/bin/env -S sh -c 'command -v node >/dev/null && exec node "$0" "$@" || exec bun "$0" "$@" '

const fs = require('fs');
const http = require('http');
const readline = require('readline');

// Agent system modules
const { executeTool, getAvailableTools, getToolsByCategory } = require('./agent-tools');
const { loadConfig, validateConfig, createDefaultConfig, getTaskTemplate, createTaskFromTemplate, getAvailableTemplates } = require('./agent-config');
const { executeTask, executeWorkflow, executeAgentWithLLM } = require('./agent-executor');

const LLM_API_BASE_URL = process.env.LLM_API_BASE_URL || 'https://api.openai.com/v1';
const LLM_API_KEY = process.env.LLM_API_KEY || process.env.OPENAI_API_KEY;
const LLM_CHAT_MODEL = process.env.LLM_CHAT_MODEL;
const LLM_STREAMING = process.env.LLM_STREAMING !== 'no';
const LLM_FORCE_REASONING = process.env.LLM_FORCE_REASONING;

const LLM_DEBUG = process.env.LLM_DEBUG;
const LLM_DEBUG_FAIL_EXIT = process.env.LLM_DEBUG_FAIL_EXIT;

// Agent mode configuration
const AGENT_MODE = process.env.AGENT_MODE === 'true' || process.env.AGENT_MODE === '1';
const AGENT_CONFIG = process.env.AGENT_CONFIG;
const AGENT_VERBOSE = process.env.AGENT_VERBOSE === 'true' || process.env.AGENT_VERBOSE === '1';

const NORMAL = '\x1b[0m';
const BOLD = '\x1b[1m';
const YELLOW = '\x1b[93m';
const MAGENTA = '\x1b[35m';
const RED = '\x1b[91m';
const GREEN = '\x1b[92m';
const CYAN = '\x1b[36m';
const GRAY = '\x1b[90m';
const ARROW = '⇢';
const CHECK = '✓';
const CROSS = '✘';

/**
 * Suspends the execution for a specified amount of time.
 *
 * @param {number} ms - The amount of time to suspend execution in milliseconds.
 * @returns {Promise<void>} - A promise that resolves after the specified time has elapsed.
 */
const sleep = async (ms) => new Promise((resolve) => setTimeout(resolve, ms));

const MAX_RETRY_ATTEMPT = 3;


/**
 * Represents a chat message.
 *
 * @typedef {Object} Message
 * @property {'system'|'user'|'assistant'} role
 * @property {string} content
 */

/**
 * A callback function to stream then completion.
 *
 * @callback CompletionHandler
 * @param {string} text
 * @returns {void}
 */

/**
 * Generates a chat completion using a RESTful LLM API service.
 *
 * @param {Array<Message>} messages - List of chat messages.
 * @param {CompletionHandler=} handler - An optional callback to stream the completion.
 * @returns {Promise<string>} The completion generated by the LLM.
 */

const chat = async (messages, handler = null, attempt = MAX_RETRY_ATTEMPT) => {
    const timeout = 17; // seconds
    const stream = LLM_STREAMING && typeof handler === 'function';
    const model = LLM_CHAT_MODEL || 'gpt-5-nano';
    const url = `${LLM_API_BASE_URL}/chat/completions`
    const auth = (LLM_API_KEY) ? { 'Authorization': `Bearer ${LLM_API_KEY}` } : {};
    const stop = ['\\boxed', '</s>', '</s>', '</s>'];

    const body = { messages, model, stop, stream }

    LLM_DEBUG &&
        messages.forEach(({ role, content }) => {
            console.log(`${MAGENTA}${role}:${NORMAL} ${content}`);
        });

    try {

        const response = await fetch(url, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json', ...auth },
            body: JSON.stringify(body),
            signal: AbortSignal.timeout(timeout * 1000)
        });
        if (!response.ok) {
            throw new Error(`HTTP error with the status: ${response.status} ${response.statusText}`);
        }

        const extract = (data) => {
            const { choices, candidates } = data;
            const first = choices ? choices[0] : candidates[0];
            if (first?.content || first?.message) {
                const content = first?.content ? first.content : first.message.content;
                const parts = content?.parts;
                const answer = parts ? parts.map(part => part.text).join('') : content;
                return answer;
            }
            return '';
        }

        if (!stream) {
            const data = await response.json();
            const answer = extract(data).trim();
            if (LLM_DEBUG) {
                console.log(`${YELLOW}${answer}${NORMAL}`);
            }
            (answer.length > 0) && handler && handler(answer);
            return answer;
        }

        const parse = (line) => {
            const separator = line.indexOf(':');
            if (separator < 0) {
                return '';
            }
            const key = line.substring(0, separator).trim();
            const payload = line.substring(separator + 1);
            if (key === 'data') {
                let partial = null;
                try {
                    const { choices } = JSON.parse(payload);
                    const [choice] = choices;
                    const { delta } = choice;
                    partial = delta?.content || '';
                } catch (e) {
                    // ignore
                } finally {
                    return partial;
                }
            } else {
                return '';
            }
        }

        const reader = response.body.getReader();
        const decoder = new TextDecoder();

        let answer = '';
        let buffer = '';
        while (true) {
            const { value, done } = await reader.read();
            if (done) {
                break;
            }
            const lines = decoder.decode(value).split('\n');
            for (let i = 0; i < lines.length; ++i) {
                const line = buffer + lines[i];
                if (line[0] === ':') {
                    buffer = '';
                    continue;
                }
                if (line === 'data: [DONE]') {
                    break;
                }
                if (line.length > 0) {
                    const partial = parse(line.trim());
                    if (partial === null) {
                        buffer = line;
                    } else if (partial && partial.length > 0) {
                        buffer = '';
                        if (answer.length < 1) {
                            const leading = partial.trim();
                            answer = leading;
                            handler && (leading.length > 0) && handler(leading);
                        } else {
                            answer += partial;
                            handler && handler(partial);
                        }
                    }
                }
            }
        }
        return answer;
    } catch (e) {
        if (e.name === 'TimeoutError') {
            LLM_DEBUG && console.log(`Timeout with LLM chat after ${timeout} seconds`);
        }
        if (attempt > 1 && (e.name === 'TimeoutError' || e.name === 'EvalError')) {
            LLM_DEBUG && console.log('Retrying...');
            await sleep((MAX_RETRY_ATTEMPT - attempt + 1) * 1500);
            return await chat(messages, handler, attempt - 1);
        } else {
            throw e;
        }
    }
}

const REPLY_PROMPT = `You are a helpful AI assistant. You are chatting with a human user.
Your answer should be a sentence or two, unless the user's request requires long-form outputs.
Never use emojis. Never use markdown. Always answer in plain text.`;

const REPLY_THINK = `You are a helpful AI assistant. You are chatting with a human user.

You should first draft your thinking process (inner monologue) until you have derived the final answer.
Write both your thoughts and answer in the same language as the task posed by the user.

Your thinking process must follow the template below:

<think>
Your thoughts or/and draft, like working through an exercise on scratch paper.
Be as casual and as long as you want until you are confident to generate a correct answer.
</think>

Your answer should be a sentence or two, unless the user's request requires long-form outputs.
Never use emojis. Never use markdown. Always answer in plain text and  in the same language as the query.`;

const reply = async (context) => {
    const { inquiry, history, delegates } = context;
    const { stream } = delegates || {};

    const messages = [];
    messages.push({ role: 'system', content: LLM_FORCE_REASONING ? REPLY_THINK : REPLY_PROMPT });
    const relevant = history.slice(-5);
    relevant.forEach(msg => {
        const { inquiry, answer } = msg;
        messages.push({ role: 'user', content: inquiry });
        messages.push({ role: 'assistant', content: answer });
    });

    messages.push({ role: 'user', content: inquiry });
    const answer = await chat(messages, stream);

    return { answer, ...context };
}

/**
 * Converts an expected answer into a suitable regular expression array.
 *
 * @param {string} match
 * @returns {Array<RegExp>}
 */
const regexify = (match) => {
    const filler = (text, index) => {
        let i = index;
        while (i < text.length) {
            if (text[i] === '/') {
                break;
            }
            ++i;
        }
        return i;
    };

    const pattern = (text, index) => {
        let i = index;
        if (text[i] === '/') {
            ++i;
            while (i < text.length) {
                if (text[i] === '/' && text[i - 1] !== '\\') {
                    break;
                }
                ++i;
            }
        }
        return i;
    };

    const regexes = [];
    let pos = 0;
    while (pos < match.length) {
        pos = filler(match, pos);
        const next = pattern(match, pos);
        if (next > pos && next < match.length) {
            const sub = match.substring(pos + 1, next);
            const regex = RegExp(sub, 'gi');
            regexes.push(regex);
            pos = next + 1;
        } else {
            break;
        }
    }

    if (regexes.length === 0) {
        regexes.push(RegExp(match, 'gi'));
    }

    return regexes;
}

/**
 * Returns all possible matches given a list of regular expressions.
 *
 * @param {string} text
 * @param {Array<RegExp>} regexes
 * @returns {Array<Span>}
 */
const match = (text, regexes) => {
    return regexes.map(regex => {
        const match = regex.exec(text);
        if (!match) {
            return null;
        }
        const [first] = match;
        const { index } = match;
        const { length } = first;
        return { index, length };
    }).filter(span => span !== null);
}

/**
 * Formats the input (using ANSI colors) to highlight the spans.
 *
 * @param {string} text
 * @param {Array<Span>} spans
 * @param {string} color
 * @returns {string}
 */

const highlight = (text, spans, color = BOLD + GREEN) => {
    let result = text;
    spans.sort((p, q) => q.index - p.index).forEach((span) => {
        const { index, length } = span;
        const prefix = result.substring(0, index);
        const content = result.substring(index, index + length);
        const suffix = result.substring(index + length);
        result = `${prefix}${color}${content}${NORMAL}${suffix}`;
    });
    return result;
}

/**
 * Evaluates a test file and executes the test cases.
 *
 * @param {string} filename - The path to the test file.
 */
const evaluate = async (filename) => {
    try {
        let history = [];
        let total = 0;
        let failures = 0;

        const handle = async (line) => {
            const parts = (line && line.length > 0) ? line.split(':') : [];
            if (parts.length >= 2) {
                const role = parts[0];
                const content = line.slice(role.length + 1).trim();
                if (role === 'Story') {
                    console.log();
                    console.log('-----------------------------------');
                    console.log(`Story: ${MAGENTA}${BOLD}${content}${NORMAL}`);
                    console.log('-----------------------------------');
                    history = [];
                } else if (role === 'User') {
                    const inquiry = content;
                    const context = { inquiry, history };
                    process.stdout.write(`  ${inquiry}\r`);
                    const start = Date.now();
                    const result = await reply(context);
                    const duration = Date.now() - start;
                    const { answer } = result;
                    history.push({ inquiry, answer: unthink(answer).trim(), duration });
                    ++total;
                } else if (role === 'Assistant') {
                    const expected = content;
                    const last = history.slice(-1).pop();
                    if (!last) {
                        console.error('There is no answer yet!');
                        process.exit(-1);
                    } else {
                        const { inquiry, answer, duration } = last;
                        const target = answer;
                        const regexes = regexify(expected);
                        const matches = match(target, regexes);
                        if (matches.length === regexes.length) {
                            console.log(`${GREEN}${CHECK} ${CYAN}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.log(' ', highlight(target, matches));
                        } else {
                            ++failures;
                            console.error(`${RED}${CROSS} ${YELLOW}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.error(`Expected ${role} to contain: ${CYAN}${regexes.join(',')}${NORMAL}`);
                            console.error(`Actual ${role}: ${MAGENTA}${target}${NORMAL}`);
                            LLM_DEBUG_FAIL_EXIT && process.exit(-1);
                        }
                    }
                }
            }
        };

        const trim = (input) => {
            const text = input.trim();
            const marker = text.indexOf('#');
            if (marker >= 0) {
                return text.substr(0, marker).trim();
            }
            return text;
        }

        const lines = fs.readFileSync(filename, 'utf-8').split('\n').map(trim);
        for (const i in lines) {
            await handle(lines[i]);
        }
        if (failures <= 0) {
            console.log(`${GREEN}${CHECK}${NORMAL} SUCCESS: ${GREEN}${total} test(s)${NORMAL}.`);
        } else {
            console.log(`${RED}${CROSS}${NORMAL} FAIL: ${GRAY}${total} test(s), ${RED}${failures} failure(s)${NORMAL}.`);
            process.exit(-1);
        }
    } catch (e) {
        console.error('ERROR:', e.toString());
        process.exit(-1);
    }
}

const THINK_START = '<think>';
const THINK_STOP = '</think>';

const unthink = (input) => {
    const start = input.indexOf(THINK_START);
    if (start < 0) {
        return input;
    }
    const end = input.indexOf(THINK_STOP);
    if (end < 0) {
        return input.substring(0, start);
    }
    return input.substring(0, start) + input.substring(end + 8);
};

const push = (display, input, threshold = THINK_START.length) => {
    let { buffer, written, print } = display;
    buffer += input;
    if (buffer.length < threshold) {
        return { buffer, written: '', print };
    }
    const incoming = unthink(buffer).trim();
    if (incoming.length > written.length) {
        const delta = incoming.substring(written.length);
        print && print(delta);
        written = incoming;
    }
    return { buffer, written, print };
};

const flush = (display) => push(display, '', 0);

/**
 * Agent-enhanced reply with tool execution capabilities
 */
const agentReply = async (context, agentConfig = null) => {
    const { inquiry, history } = context;
    
    // Check if inquiry is a special agent command
    if (inquiry.startsWith('/agent ')) {
        const command = inquiry.substring(7).trim();
        return await handleAgentCommand(command, agentConfig);
    }
    
    // If agent mode and config available, use agent executor
    if (AGENT_MODE && agentConfig) {
        try {
            const config = { verbose: AGENT_VERBOSE };
            const result = await executeAgentWithLLM(agentConfig, inquiry, chat, config);
            
            if (result.success) {
                const answer = formatAgentResult(result);
                return { answer, ...context };
            } else {
                // Fall back to regular reply on agent failure
                console.log(`${YELLOW}Agent execution failed, falling back to regular mode${NORMAL}`);
            }
        } catch (error) {
            console.error(`${RED}Agent error: ${error.message}${NORMAL}`);
        }
    }
    
    // Fall back to regular reply
    return await reply(context);
};

/**
 * Handle special agent commands
 */
const handleAgentCommand = async (command, agentConfig) => {
    if (command === 'help') {
        const tools = getToolsByCategory();
        let helpText = 'Available agent commands:\n';
        helpText += '/agent help - Show this help\n';
        helpText += '/agent tools - List all available tools\n';
        helpText += '/agent config - Show current agent configuration\n';
        helpText += '/agent templates - List available task templates\n\n';
        helpText += 'Available tool categories:\n';
        for (const [category, toolList] of Object.entries(tools)) {
            helpText += `  ${category}: ${toolList.join(', ')}\n`;
        }
        return { answer: helpText };
    } else if (command === 'tools') {
        const tools = getAvailableTools();
        return { answer: `Available tools: ${tools.join(', ')}` };
    } else if (command === 'config') {
        if (agentConfig) {
            return { answer: `Current agent: ${agentConfig.name}\n${agentConfig.description || ''}` };
        } else {
            return { answer: 'No agent configuration loaded' };
        }
    } else if (command === 'templates') {
        const templates = getAvailableTemplates();
        return { answer: `Available templates: ${templates.join(', ')}` };
    } else {
        return { answer: 'Unknown agent command. Try /agent help' };
    }
};

/**
 * Format agent execution result for display
 */
const formatAgentResult = (result) => {
    if (result.context && Object.keys(result.context).length > 0) {
        // Find the most relevant result to display
        const lastResult = result.taskResults?.[result.taskResults.length - 1];
        if (lastResult && lastResult.results) {
            const finalStep = lastResult.results[lastResult.results.length - 1];
            if (finalStep && finalStep.result !== undefined) {
                if (typeof finalStep.result === 'object') {
                    return JSON.stringify(finalStep.result, null, 2);
                }
                return String(finalStep.result);
            }
        }
    }
    return 'Task completed successfully';
};

/**
 * Represents the contextual information for each pipeline stage.
 *
 * @typedef {Object} Context
 * @property {Array<object>} history
 * @property {string} inquiry
 * @property {string} answer
 * @property {Object.<string, function>} delegates - Impure functions to access the outside world.
 */

const interact = async (agentConfig = null) => {
    const history = [];

    let loop = true;
    const io = readline.createInterface({ input: process.stdin, output: process.stdout });
    io.on('close', () => { loop = false; });

    const qa = () => {
        io.question(`${YELLOW}>> ${CYAN}`, async (inquiry) => {
            process.stdout.write(NORMAL);
            const print = (text) => process.stdout.write(text);
            let display = { buffer: '', written: '', print };
            const stream = (text) => display = push(display, text);
            const delegates = { stream };
            const context = { inquiry, history, delegates };
            const start = Date.now();
            
            // Use agentReply if agent mode is enabled, otherwise use regular reply
            const result = AGENT_MODE 
                ? await agentReply(context, agentConfig) 
                : await reply(context);
            
            const duration = Date.now() - start;
            display = flush(display);
            const answer = result.answer || display.written;
            history.push({ inquiry, answer, duration });
            console.log(answer);
            console.log();
            loop && qa();
        })
    }

    qa();
}

/**
 * Starts an HTTP server that listens on the specified port and serves requests.
 *
 * @param {number} port - The port number to listen on.
 */
const serve = async (port) => {
    let history = [];

    const decode = (url) => {
        const parsedUrl = new URL(`http://localhost/${url}`);
        const { search } = parsedUrl;
        return decodeURIComponent(search.substring(1)).trim();
    };

    const server = http.createServer(async (request, response) => {
        const { url } = request;
        if (url === '/health') {
            response.writeHead(200).end('OK');
        } else if (url === '/' || url === '/index.html') {
            response.writeHead(200, { 'Content-Type': 'text/html' });
            response.end(fs.readFileSync('./index.html'));
        } else if (url.startsWith('/chat')) {
            const inquiry = decode(url);
            if (inquiry === '/reset') {
                history = [];
                response.write('History cleared.');
                response.end();
            } else if (inquiry.length > 0) {
                console.log(`${YELLOW}>> ${CYAN}${inquiry}${NORMAL}`);
                response.writeHead(200, { 'Content-Type': 'text/plain' });

                const stream = (text) => {
                    process.stdout.write(text);
                    response.write(text);
                }
                const delegates = { stream };
                const context = { inquiry, history, delegates };
                const start = Date.now();
                const { answer } = await reply(context);
                console.log();
                response.end();
                const duration = Date.now() - start;
                history.push({ inquiry, answer, duration });
            } else {
                response.writeHead(400).end();
            }
        } else {
            console.error(`${url} is 404!`);
            response.writeHead(404);
            response.end();
        }
    });
    server.listen(port);
    console.log('Listening on port', port);
};

const canary = async () => {
    console.log(`Using LLM at ${YELLOW}${LLM_API_BASE_URL}${NORMAL} (model: ${GREEN}${LLM_CHAT_MODEL || 'default'}${NORMAL}).`);
    process.stdout.write(`${ARROW} Checking LLM...\r`);

    const inquiry = 'What is the capital of France?';
    const history = [];
    const context = { inquiry, history };
    try {
        const { answer } = await reply(context);
        LLM_REASONING_ABILITY = answer.includes(THINK_START) && answer.includes(THINK_STOP);
        console.log(`LLM is ${GREEN}ready${NORMAL} (working as expected).`);
        if (LLM_REASONING_ABILITY) {
            console.log(`This is a ${YELLOW}reasoning${NORMAL} model.`);
        } else {
            console.log(`This is a regular model, ${MAGENTA}not${NORMAL} capable of self-reasoning.`);
        }
        console.log();
    } catch (error) {
        console.error(`${CROSS} ${RED}Fatal error: LLM is not ready!${NORMAL}`);
        console.error(error);
        process.exit(-1);
    }
};

(async () => {
    // Load agent configuration if specified
    let agentConfig = null;
    if (AGENT_MODE) {
        if (AGENT_CONFIG && fs.existsSync(AGENT_CONFIG)) {
            try {
                agentConfig = loadConfig(AGENT_CONFIG);
                validateConfig(agentConfig);
                console.log(`${GREEN}${CHECK}${NORMAL} Loaded agent configuration: ${CYAN}${agentConfig.name}${NORMAL}`);
                if (agentConfig.description) {
                    console.log(`  ${agentConfig.description}`);
                }
            } catch (error) {
                console.error(`${RED}${CROSS}${NORMAL} Failed to load agent config: ${error.message}`);
                process.exit(-1);
            }
        } else {
            console.log(`${YELLOW}Agent mode enabled without configuration file${NORMAL}`);
            console.log(`Use AGENT_CONFIG=/path/to/config.json to load a configuration`);
            agentConfig = createDefaultConfig();
        }
        console.log();
    }

    await canary();

    const args = process.argv.slice(2);
    args.forEach(evaluate);
    if (args.length == 0) {
        const port = parseInt(process.env.HTTP_PORT, 10);
        if (!Number.isNaN(port) && port > 0 && port < 65536) {
            await serve(port);
        } else {
            await interact(agentConfig);
        }
    }
})();
