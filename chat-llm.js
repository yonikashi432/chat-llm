#!/usr/bin/env -S sh -c 'command -v node >/dev/null && exec node "$0" "$@" || exec bun "$0" "$@" '

const fs = require('fs');
const http = require('http');
const readline = require('readline');
const { analyzeSentiment } = require('./tools/sentiment_analyzer');
const { RequestLogger } = require('./tools/request-logger');

// Agent system modules
const { executeTool, getAvailableTools, getToolsByCategory } = require('./agent-tools');
const { loadConfig, validateConfig, createDefaultConfig, getTaskTemplate, createTaskFromTemplate, getAvailableTemplates } = require('./agent-config');
const { executeTask, executeWorkflow, executeAgentWithLLM } = require('./agent-executor');

const LLM_API_BASE_URL = process.env.LLM_API_BASE_URL || 'https://api.openai.com/v1';
const LLM_API_KEY = process.env.LLM_API_KEY || process.env.OPENAI_API_KEY;
const LLM_CHAT_MODEL = process.env.LLM_CHAT_MODEL;
const LLM_STREAMING = process.env.LLM_STREAMING !== 'no';
const LLM_FORCE_REASONING = process.env.LLM_FORCE_REASONING;

let LLM_MODEL_CONFIGS = {};
if (process.env.LLM_MODEL_CONFIGS) {
    try {
        LLM_MODEL_CONFIGS = JSON.parse(process.env.LLM_MODEL_CONFIGS);
    } catch (e) {
        console.error(`${RED}Error parsing LLM_MODEL_CONFIGS: ${e.message}${NORMAL}`);
        process.exit(-1);
    }
}

const LLM_DEBUG = process.env.LLM_DEBUG;
const LLM_DEBUG_FAIL_EXIT = process.env.LLM_DEBUG_FAIL_EXIT;

// Agent mode configuration
const AGENT_MODE = process.env.AGENT_MODE === 'true' || process.env.AGENT_MODE === '1';
const AGENT_CONFIG = process.env.AGENT_CONFIG;
const AGENT_VERBOSE = process.env.AGENT_VERBOSE === 'true' || process.env.AGENT_VERBOSE === '1';

const NORMAL = '\x1b[0m';
const BOLD = '\x1b[1m';
const YELLOW = '\x1b[93m';
const MAGENTA = '\x1b[35m';
const RED = '\x1b[91m';
const GREEN = '\x1b[92m';
const CYAN = '\x1b[36m';
const GRAY = '\x1b[90m';
const ARROW = '⇢';
const CHECK = '✓';
const CROSS = '✘';

/**
 * Suspends the execution for a specified amount of time.
 *
 * @param {number} ms - The amount of time to suspend execution in milliseconds.
 * @returns {Promise<void>} - A promise that resolves after the specified time has elapsed.
 */
const sleep = async (ms) => new Promise((resolve) => setTimeout(resolve, ms));

const MAX_RETRY_ATTEMPT = 3;
const logger = new RequestLogger('./logs');



/**
 * Represents a chat message.
 *
 * @typedef {Object} Message
 * @property {'system'|'user'|'assistant'} role
 * @property {string} content
 */

/**
 * A callback function to stream then completion.
 *
 * @callback CompletionHandler
 * @param {string} text
 * @returns {void}
 */

/**
 * Generates a chat completion using a RESTful LLM API service.
 *
 * @param {Array<Message>} messages - List of chat messages.
 * @param {CompletionHandler=} handler - An optional callback to stream the completion.
 * @returns {Promise<string>} The completion generated by the LLM.
 */

const chat = async (messages, handler = null, attempt = MAX_RETRY_ATTEMPT, modelName = null) => {
    const timeout = 17; // seconds
    const stream = LLM_STREAMING && typeof handler === 'function';

    let apiBaseUrl = LLM_API_BASE_URL;
    let apiKey = LLM_API_KEY;
    let chatModel = LLM_CHAT_MODEL || 'gpt-5-nano';

    if (modelName && LLM_MODEL_CONFIGS[modelName]) {
        const config = LLM_MODEL_CONFIGS[modelName];
        apiBaseUrl = config.api_base_url || apiBaseUrl;
        apiKey = config.api_key || apiKey;
        chatModel = config.chat_model || chatModel;
    }

    const url = `${apiBaseUrl}/chat/completions`
    const auth = (apiKey) ? { 'Authorization': `Bearer ${apiKey}` } : {};
    const stop = ['\\boxed', '</s>', '</s>', '</s>'];

    const body = { messages, model: chatModel, stop, stream }

    LLM_DEBUG &&
        messages.forEach(({ role, content }) => {
            console.log(`${MAGENTA}${role}:${NORMAL} ${content}`);
        });

    try {

        const response = await fetch(url, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json', ...auth },
            body: JSON.stringify(body),
            signal: AbortSignal.timeout(timeout * 1000)
        });
        if (!response.ok) {
            const status = response.status;
            const statusText = response.statusText;
            
            // Handle rate limiting with exponential backoff
            if (status === 429 && attempt > 1) {
                const waitTime = (MAX_RETRY_ATTEMPT - attempt + 2) * 5000; // 5s, 10s, 15s
                LLM_DEBUG && console.log(`Rate limited (429). Waiting ${waitTime}ms before retry...`);
                await sleep(waitTime);
                return await chat(messages, handler, attempt - 1);
            }
            
            throw new Error(`HTTP error with the status: ${status} ${statusText}`);
        }

        const extract = (data) => {
            const { choices, candidates } = data;
            const first = choices ? choices[0] : candidates[0];
            if (first?.content || first?.message) {
                const content = first?.content ? first.content : first.message.content;
                const parts = content?.parts;
                const answer = parts ? parts.map(part => part.text).join('') : content;
                return answer;
            }
            return '';
        }

        if (!stream) {
            const data = await response.json();
            const answer = extract(data).trim();
            if (LLM_DEBUG) {
                console.log(`${YELLOW}${answer}${NORMAL}`);
            }
            (answer.length > 0) && handler && handler(answer);
            return answer;
        }

        const parse = (line) => {
            const separator = line.indexOf(':');
            if (separator < 0) {
                return '';
            }
            const key = line.substring(0, separator).trim();
            const payload = line.substring(separator + 1);
            if (key === 'data') {
                let partial = null;
                try {
                    const { choices } = JSON.parse(payload);
                    const [choice] = choices;
                    const { delta } = choice;
                    partial = delta?.content || '';
                } catch (e) {
                    // ignore
                } finally {
                    return partial;
                }
            } else {
                return '';
            }
        }

        const reader = response.body.getReader();
        const decoder = new TextDecoder();

        let answer = '';
        let buffer = '';
        while (true) {
            const { value, done } = await reader.read();
            if (done) {
                break;
            }
            const lines = decoder.decode(value).split('\n');
            for (let i = 0; i < lines.length; ++i) {
                const line = buffer + lines[i];
                if (line[0] === ':') {
                    buffer = '';
                    continue;
                }
                if (line === 'data: [DONE]') {
                    break;
                }
                if (line.length > 0) {
                    const partial = parse(line.trim());
                    if (partial === null) {
                        buffer = line;
                    } else if (partial && partial.length > 0) {
                        buffer = '';
                        if (answer.length < 1) {
                            const leading = partial.trim();
                            answer = leading;
                            handler && (leading.length > 0) && handler(leading);
                        } else {
                            answer += partial;
                            handler && handler(partial);
                        }
                    }
                }
            }
        }
        return answer;
    } catch (e) {
        if (e.name === 'TimeoutError') {
            LLM_DEBUG && console.log(`Timeout with LLM chat after ${timeout} seconds`);
        }
        if (attempt > 1 && (e.name === 'TimeoutError' || e.name === 'EvalError')) {
            LLM_DEBUG && console.log('Retrying...');
            await sleep((MAX_RETRY_ATTEMPT - attempt + 1) * 1500);
            return await chat(messages, handler, attempt - 1);
        } else {
            throw e;
        }
    }
}

const REPLY_PROMPT = `You are a helpful AI assistant. You are chatting with a human user.
Your answer should be a sentence or two, unless the user's request requires long-form outputs.
Never use emojis. Never use markdown. Always answer in plain text.`;

const REPLY_THINK = `You are a helpful AI assistant. You are chatting with a human user.

You have access to the following tools:
- analyzeSentiment(text: string): Analyzes the sentiment of the given text. Returns an object with 'sentiment' (positive, negative, neutral) and 'score'.

You can also select a specific LLM model to use for your response.
Available models are: ${Object.keys(LLM_MODEL_CONFIGS).length > 0 ? Object.keys(LLM_MODEL_CONFIGS).join(', ') : 'None configured, using default.'}
To use a specific model, include <use_model>model_name</use_model> in your prompt.

You should first draft your thinking process (inner monologue) until you have derived the final answer.
Write both your thoughts and answer in the same language as the task posed by the user.

Your thinking process must follow the template below:

<think>
Your thoughts or/and draft, like working through an exercise on scratch paper.
Be as casual and as long as you want until you are confident to generate a correct answer.
If you need to use a tool, use the following format:
<tool_code>
console.log(analyzeSentiment("text to analyze"));
</tool_code>
</think>

Your answer should be a sentence or two, unless the user's request requires long-form outputs.
Never use emojis. Never use markdown. Always answer in plain text and  in the same language as the query.`;

const DEMO_RESPONSES = {
    'capital': ['Paris is the capital of France.', 'The capital of France is Paris.'],
    'weather': ['I cannot check the current weather, but you can check a weather service online.', 'Weather information is not available in demo mode.'],
    'time': ['I cannot provide real-time information in demo mode.', 'Time-based queries are not supported in demo mode.'],
    'planet': ['Jupiter is the largest planet in our solar system.', 'The largest planet is Jupiter.'],
    'smallest': ['Mercury is the smallest planet in our solar system.', 'The smallest planet is Mercury.'],
    'hottest': ['Venus is the hottest planet in our solar system.', 'The hottest planet is Venus.'],
    'default': ['This is a demo response since the LLM API is unavailable.', 'I am in demo mode and cannot provide real responses.', 'Demo mode is active - API responses are simulated.']
};

const demoReply = async (inquiry) => {
    // Simple pattern matching for demo responses
    const lowerInquiry = inquiry.toLowerCase();
    
    if (lowerInquiry.includes('capital') || lowerInquiry.includes('france') || lowerInquiry.includes('paris')) {
        return DEMO_RESPONSES.capital[Math.floor(Math.random() * DEMO_RESPONSES.capital.length)];
    } else if (lowerInquiry.includes('weather') || lowerInquiry.includes('rain') || lowerInquiry.includes('temperature')) {
        return DEMO_RESPONSES.weather[Math.floor(Math.random() * DEMO_RESPONSES.weather.length)];
    } else if (lowerInquiry.includes('time') || lowerInquiry.includes('hour') || lowerInquiry.includes('clock')) {
        return DEMO_RESPONSES.time[Math.floor(Math.random() * DEMO_RESPONSES.time.length)];
    } else if (lowerInquiry.includes('largest') && lowerInquiry.includes('planet')) {
        return DEMO_RESPONSES.planet[Math.floor(Math.random() * DEMO_RESPONSES.planet.length)];
    } else if (lowerInquiry.includes('smallest') && lowerInquiry.includes('planet')) {
        return DEMO_RESPONSES.smallest[Math.floor(Math.random() * DEMO_RESPONSES.smallest.length)];
    } else if (lowerInquiry.includes('hottest') && lowerInquiry.includes('planet')) {
        return DEMO_RESPONSES.hottest[Math.floor(Math.random() * DEMO_RESPONSES.hottest.length)];
    }
    
    return DEMO_RESPONSES.default[Math.floor(Math.random() * DEMO_RESPONSES.default.length)];
};

const reply = async (context) => {
    const { inquiry, history, delegates } = context;
    const { stream } = delegates || {};

    const messages = [];
    messages.push({ role: 'system', content: LLM_FORCE_REASONING ? REPLY_THINK : REPLY_PROMPT });
    const relevant = history.slice(-5);
    relevant.forEach(msg => {
        const { inquiry, answer } = msg;
        messages.push({ role: 'user', content: inquiry });
        messages.push({ role: 'assistant', content: answer });
    });

    messages.push({ role: 'user', content: inquiry });

    let selectedModel = null;
    // Check if the user explicitly requested a model in the inquiry
    const modelMatch = inquiry.match(/<use_model>(.*?)<\/use_model>/);
    if (modelMatch && modelMatch[1]) {
        selectedModel = modelMatch[1].trim();
    }
    
    let rawAnswer;
    try {
        rawAnswer = await chat(messages, stream, MAX_RETRY_ATTEMPT, selectedModel);
    } catch (error) {
        if (process.env.LLM_DEMO_MODE) {
            rawAnswer = await demoReply(inquiry);
        } else {
            throw error;
        }
    }

    const THINK_START_TAG = '<think>';
    const THINK_STOP_TAG = '</think>';
    const TOOL_CODE_START_TAG = '<tool_code>';
    const TOOL_CODE_STOP_TAG = '</tool_code>';

    const thinkStartIndex = rawAnswer.indexOf(THINK_START_TAG);
    const thinkStopIndex = rawAnswer.indexOf(THINK_STOP_TAG);


    if (thinkStartIndex !== -1 && thinkStopIndex !== -1) {
        const thinkContent = rawAnswer.substring(thinkStartIndex + THINK_START_TAG.length, thinkStopIndex);
        const toolCodeStartIndex = thinkContent.indexOf(TOOL_CODE_START_TAG);
        const toolCodeStopIndex = thinkContent.indexOf(TOOL_CODE_STOP_TAG);

        if (toolCodeStartIndex !== -1 && toolCodeStopIndex !== -1) {
            const toolCode = thinkContent.substring(toolCodeStartIndex + TOOL_CODE_START_TAG.length, toolCodeStopIndex);
            let toolOutput = '';
            try {
                // Execute the tool code in a sandboxed environment
                // For simplicity, using eval here. In a real-world scenario, consider a more secure sandbox.
                const consoleLog = (output) => { toolOutput += JSON.stringify(output) + '\n'; };
                const context = { analyzeSentiment, console: { log: consoleLog } };
                const script = new Function('analyzeSentiment', 'console', toolCode);
                script(context.analyzeSentiment, context.console);
            } catch (e) {
                toolOutput = `Error executing tool: ${e.message}`;
            }

            // Re-prompt the LLM with the tool output
            messages.push({ role: 'assistant', content: rawAnswer }); // Add the LLM's initial thought process
            messages.push({ role: 'system', content: `Tool output:\n${toolOutput}` });
            messages.push({ role: 'user', content: 'Given the tool output, provide your final answer.' });
            rawAnswer = await chat(messages, stream);
        }
    }

    return { answer: rawAnswer, ...context };
}

/**
 * Converts an expected answer into a suitable regular expression array.
 *
 * @param {string} match
 * @returns {Array<RegExp>}
 */
const regexify = (match) => {
    const filler = (text, index) => {
        let i = index;
        while (i < text.length) {
            if (text[i] === '/') {
                break;
            }
            ++i;
        }
        return i;
    };

    const pattern = (text, index) => {
        let i = index;
        if (text[i] === '/') {
            ++i;
            while (i < text.length) {
                if (text[i] === '/' && text[i - 1] !== '\\') {
                    break;
                }
                ++i;
            }
        }
        return i;
    };

    const regexes = [];
    let pos = 0;
    while (pos < match.length) {
        pos = filler(match, pos);
        const next = pattern(match, pos);
        if (next > pos && next < match.length) {
            const sub = match.substring(pos + 1, next);
            const regex = RegExp(sub, 'gi');
            regexes.push(regex);
            pos = next + 1;
        } else {
            break;
        }
    }

    if (regexes.length === 0) {
        regexes.push(RegExp(match, 'gi'));
    }

    return regexes;
}

/**
 * Returns all possible matches given a list of regular expressions.
 *
 * @param {string} text
 * @param {Array<RegExp>} regexes
 * @returns {Array<Span>}
 */
const match = (text, regexes) => {
    return regexes.map(regex => {
        const match = regex.exec(text);
        if (!match) {
            return null;
        }
        const [first] = match;
        const { index } = match;
        const { length } = first;
        return { index, length };
    }).filter(span => span !== null);
}

/**
 * Formats the input (using ANSI colors) to highlight the spans.
 *
 * @param {string} text
 * @param {Array<Span>} spans
 * @param {string} color
 * @returns {string}
 */

const highlight = (text, spans, color = BOLD + GREEN) => {
    let result = text;
    spans.sort((p, q) => q.index - p.index).forEach((span) => {
        const { index, length } = span;
        const prefix = result.substring(0, index);
        const content = result.substring(index, index + length);
        const suffix = result.substring(index + length);
        result = `${prefix}${color}${content}${NORMAL}${suffix}`;
    });
    return result;
}

/**
 * Evaluates a test file and executes the test cases.
 *
 * @param {string} filename - The path to the test file.
 */
const evaluate = async (filename) => {
    try {
        let history = [];
        let total = 0;
        let failures = 0;

        const handle = async (line) => {
            const parts = (line && line.length > 0) ? line.split(':') : [];
            if (parts.length >= 2) {
                const role = parts[0];
                const content = line.slice(role.length + 1).trim();
                if (role === 'Story') {
                    console.log();
                    console.log('-----------------------------------');
                    console.log(`Story: ${MAGENTA}${BOLD}${content}${NORMAL}`);
                    console.log('-----------------------------------');
                    history = [];
                } else if (role === 'User') {
                    const inquiry = content;
                    const context = { inquiry, history };
                    process.stdout.write(`  ${inquiry}\r`);
                    const start = Date.now();
                    const result = await reply(context);
                    const duration = Date.now() - start;
                    const { answer } = result;
                    history.push({ inquiry, answer: unthink(answer).trim(), duration });
                    ++total;
                } else if (role === 'Assistant') {
                    const expected = content;
                    const last = history.slice(-1).pop();
                    if (!last) {
                        console.error('There is no answer yet!');
                        process.exit(-1);
                    } else {
                        const { inquiry, answer, duration } = last;
                        const target = answer;
                        const regexes = regexify(expected);
                        const matches = match(target, regexes);
                        if (matches.length === regexes.length) {
                            console.log(`${GREEN}${CHECK} ${CYAN}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.log(' ', highlight(target, matches));
                        } else {
                            ++failures;
                            console.error(`${RED}${CROSS} ${YELLOW}${inquiry} ${GRAY}[${duration} ms]${NORMAL}`);
                            console.error(`Expected ${role} to contain: ${CYAN}${regexes.join(',')}${NORMAL}`);
                            console.error(`Actual ${role}: ${MAGENTA}${target}${NORMAL}`);
                            LLM_DEBUG_FAIL_EXIT && process.exit(-1);
                        }
                    }
                }
            }
        };

        const trim = (input) => {
            const text = input.trim();
            const marker = text.indexOf('#');
            if (marker >= 0) {
                return text.substr(0, marker).trim();
            }
            return text;
        }

        const lines = fs.readFileSync(filename, 'utf-8').split('\n').map(trim);
        for (const i in lines) {
            await handle(lines[i]);
        }
        if (failures <= 0) {
            console.log(`${GREEN}${CHECK}${NORMAL} SUCCESS: ${GREEN}${total} test(s)${NORMAL}.`);
        } else {
            console.log(`${RED}${CROSS}${NORMAL} FAIL: ${GRAY}${total} test(s), ${RED}${failures} failure(s)${NORMAL}.`);
            process.exit(-1);
        }
    } catch (e) {
        console.error('ERROR:', e.toString());
        process.exit(-1);
    }
}

const THINK_START = '<think>';
const THINK_STOP = '</think>';

const unthink = (input) => {
    const start = input.indexOf(THINK_START);
    if (start < 0) {
        return input;
    }
    const end = input.indexOf(THINK_STOP);
    if (end < 0) {
        return input.substring(0, start);
    }
    return input.substring(0, start) + input.substring(end + 8);
};

const push = (display, input, threshold = THINK_START.length) => {
    let { buffer, written, print } = display;
    buffer += input;
    if (buffer.length < threshold) {
        return { buffer, written: '', print };
    }
    const incoming = unthink(buffer).trim();
    if (incoming.length > written.length) {
        const delta = incoming.substring(written.length);
        print && print(delta);
        written = incoming;
    }
    return { buffer, written, print };
};

const flush = (display) => push(display, '', 0);

/**
 * Agent-enhanced reply with tool execution capabilities
 */
const agentReply = async (context, agentConfig = null) => {
    const { inquiry, history } = context;
    
    // Check if inquiry is a special agent command
    if (inquiry.startsWith('/agent ')) {
        const command = inquiry.substring(7).trim();
        return await handleAgentCommand(command, agentConfig);
    }
    
    // If agent mode and config available, use agent executor
    if (AGENT_MODE && agentConfig) {
        try {
            const config = { verbose: AGENT_VERBOSE };
            const result = await executeAgentWithLLM(agentConfig, inquiry, chat, config);
            
            if (result.success) {
                const answer = formatAgentResult(result);
                return { answer, ...context };
            } else {
                // Fall back to regular reply on agent failure
                console.log(`${YELLOW}Agent execution failed, falling back to regular mode${NORMAL}`);
            }
        } catch (error) {
            console.error(`${RED}Agent error: ${error.message}${NORMAL}`);
        }
    }
    
    // Fall back to regular reply
    return await reply(context);
};

/**
 * Handle special agent commands
 */
const handleAgentCommand = async (command, agentConfig) => {
    if (command === 'help') {
        const tools = getToolsByCategory();
        let helpText = 'Available agent commands:\n';
        helpText += '/agent help - Show this help\n';
        helpText += '/agent tools - List all available tools\n';
        helpText += '/agent config - Show current agent configuration\n';
        helpText += '/agent templates - List available task templates\n\n';
        helpText += 'Available tool categories:\n';
        for (const [category, toolList] of Object.entries(tools)) {
            helpText += `  ${category}: ${toolList.join(', ')}\n`;
        }
        return { answer: helpText };
    } else if (command === 'tools') {
        const tools = getAvailableTools();
        return { answer: `Available tools: ${tools.join(', ')}` };
    } else if (command === 'config') {
        if (agentConfig) {
            return { answer: `Current agent: ${agentConfig.name}\n${agentConfig.description || ''}` };
        } else {
            return { answer: 'No agent configuration loaded' };
        }
    } else if (command === 'templates') {
        const templates = getAvailableTemplates();
        return { answer: `Available templates: ${templates.join(', ')}` };
    } else {
        return { answer: 'Unknown agent command. Try /agent help' };
    }
};

/**
 * Format agent execution result for display
 */
const formatAgentResult = (result) => {
    if (result.context && Object.keys(result.context).length > 0) {
        // Find the most relevant result to display
        const lastResult = result.taskResults?.[result.taskResults.length - 1];
        if (lastResult && lastResult.results) {
            const finalStep = lastResult.results[lastResult.results.length - 1];
            if (finalStep && finalStep.result !== undefined) {
                if (typeof finalStep.result === 'object') {
                    return JSON.stringify(finalStep.result, null, 2);
                }
                return String(finalStep.result);
            }
        }
    }
    return 'Task completed successfully';
};

/**
 * Represents the contextual information for each pipeline stage.
 *
 * @typedef {Object} Context
 * @property {Array<object>} history
 * @property {string} inquiry
 * @property {string} answer
 * @property {Object.<string, function>} delegates - Impure functions to access the outside world.
 */

const interact = async (agentConfig = null) => {
    const history = [];

    let loop = true;
    const io = readline.createInterface({ input: process.stdin, output: process.stdout });
    io.on('close', () => { loop = false; });

    const qa = () => {
        io.question(`${YELLOW}>> ${CYAN}`, async (inquiry) => {
            process.stdout.write(NORMAL);
            const print = (text) => process.stdout.write(text);
            let display = { buffer: '', written: '', print };
            const stream = (text) => display = push(display, text);
            const delegates = { stream };
            const context = { inquiry, history, delegates };
            const start = Date.now();
            
            // Use agentReply if agent mode is enabled, otherwise use regular reply
            const result = AGENT_MODE 
                ? await agentReply(context, agentConfig) 
                : await reply(context);
            
            const duration = Date.now() - start;
            display = flush(display);
            const answer = result.answer || display.written;
            history.push({ inquiry, answer, duration });
            
            // Only print if answer wasn't already streamed (agent mode direct responses)
            if (AGENT_MODE && result.answer) {
                console.log(answer);
            }
            console.log();
            loop && qa();
        })
    }

    qa();
}

/**
 * Starts an HTTP server that listens on the specified port and serves requests.
 *
 * @param {number} port - The port number to listen on.
 */
const serve = async (port) => {
    let history = [];

    const decode = (url) => {
        const parsedUrl = new URL(`http://localhost/${url}`);
        const { search } = parsedUrl;
        return decodeURIComponent(search.substring(1)).trim();
    };

    const server = http.createServer(async (request, response) => {
        const { url } = request;
        if (url === '/health') {
            response.writeHead(200).end('OK');
        } else if (url === '/' || url === '/index.html') {
            response.writeHead(200, { 'Content-Type': 'text/html' });
            response.end(fs.readFileSync('./index.html'));
        } else if (url.startsWith('/chat')) {
            const inquiry = decode(url);
            if (inquiry === '/reset') {
                history = [];
                response.write('History cleared.');
                response.end();
            } else if (inquiry.length > 0) {
                console.log(`${YELLOW}>> ${CYAN}${inquiry}${NORMAL}`);
                response.writeHead(200, { 'Content-Type': 'text/plain' });

                const stream = (text) => {
                    process.stdout.write(text);
                    response.write(text);
                }
                const delegates = { stream };
                const context = { inquiry, history, delegates };
                const start = Date.now();
                const { answer } = await reply(context);
                console.log();
                response.end();
                const duration = Date.now() - start;
                history.push({ inquiry, answer, duration });
            } else {
                response.writeHead(400).end();
            }
        } else {
            console.error(`${url} is 404!`);
            response.writeHead(404);
            response.end();
        }
    });
    server.listen(port);
    console.log('Listening on port', port);
};

const canary = async () => {
    console.log(`Using LLM at ${YELLOW}${LLM_API_BASE_URL}${NORMAL} (model: ${GREEN}${LLM_CHAT_MODEL || 'default'}${NORMAL}).`);
    process.stdout.write(`${ARROW} Checking LLM...\r`);

    const inquiry = 'What is the capital of France?';
    const history = [];
    const context = { inquiry, history };
    try {
        const { answer } = await reply(context);
        LLM_REASONING_ABILITY = answer.includes(THINK_START) && answer.includes(THINK_STOP);
        console.log(`LLM is ${GREEN}ready${NORMAL} (working as expected).`);
        if (LLM_REASONING_ABILITY) {
            console.log(`This is a ${YELLOW}reasoning${NORMAL} model.`);
        } else {
            console.log(`This is a regular model, ${MAGENTA}not${NORMAL} capable of self-reasoning.`);
        }
        console.log();
    } catch (error) {
        if (process.env.LLM_DEMO_MODE) {
            console.log(`${YELLOW}⚠${NORMAL}  LLM unavailable - running in ${CYAN}demo mode${NORMAL}.`);
            console.log('   (Responses will be simulated for testing purposes)\n');
            return;
        }
        console.error(`${CROSS} ${RED}Fatal error: LLM is not ready!${NORMAL}`);
        console.error(error);
        process.exit(-1);
    }
};

(async () => {
    // Load agent configuration if specified
    let agentConfig = null;
    if (AGENT_MODE) {
        if (AGENT_CONFIG && fs.existsSync(AGENT_CONFIG)) {
            try {
                agentConfig = loadConfig(AGENT_CONFIG);
                validateConfig(agentConfig);
                console.log(`${GREEN}${CHECK}${NORMAL} Loaded agent configuration: ${CYAN}${agentConfig.name}${NORMAL}`);
                if (agentConfig.description) {
                    console.log(`  ${agentConfig.description}`);
                }
            } catch (error) {
                console.error(`${RED}${CROSS}${NORMAL} Failed to load agent config: ${error.message}`);
                process.exit(-1);
            }
        } else {
            console.log(`${YELLOW}Agent mode enabled without configuration file${NORMAL}`);
            console.log(`Use AGENT_CONFIG=/path/to/config.json to load a configuration`);
            agentConfig = createDefaultConfig();
        }
        console.log();
    }

    await canary();

    const args = process.argv.slice(2);
    
    if (args.length === 0 || args[0] === '--help' || args[0] === '-h' || args[0] === 'help') {
        console.log(`${BOLD}Chat LLM${NORMAL} - Zero-dependency LLM chat tool\n`);
        console.log(`${CYAN}Usage:${NORMAL}`);
        console.log(`  ./chat-llm.js                              # Interactive mode`);
        console.log(`  ./chat-llm.js <test-file>                 # Run test file`);
        console.log(`  ./chat-llm.js sentiment <text>            # Analyze sentiment`);
        console.log(`  ./chat-llm.js stats                       # Show request statistics`);
        console.log(`  ./chat-llm.js export <format>             # Export logs (json|csv)`);
        console.log(`  HTTP_PORT=5000 ./chat-llm.js              # Web interface\n`);
        console.log(`${CYAN}Environment Variables:${NORMAL}`);
        console.log(`  LLM_API_BASE_URL         # API endpoint (default: OpenAI)`);
        console.log(`  LLM_API_KEY              # API authentication key`);
        console.log(`  LLM_CHAT_MODEL           # Model name to use`);
        console.log(`  LLM_STREAMING            # Enable streaming (default: yes)`);
        console.log(`  LLM_FORCE_REASONING      # Use reasoning mode`);
        console.log(`  LLM_DEMO_MODE            # Run in demo mode`);
        console.log(`  HTTP_PORT                # Enable web server on port\n`);
        process.exit(0);
    } else if (args[0] === 'sentiment' && args.length > 1) {
        const textToAnalyze = args.slice(1).join(' ');
        const start = Date.now();
        const result = analyzeSentiment(textToAnalyze);
        const duration = Date.now() - start;
        console.log(JSON.stringify(result, null, 2));
        logger.logRequest('sentiment', textToAnalyze, JSON.stringify(result), duration);
    } else if (args[0] === 'stats') {
        const stats = logger.getStats();
        if (stats) {
            console.log(JSON.stringify(stats, null, 2));
        } else {
            console.log('No request logs found.');
        }
    } else if (args[0] === 'export' && args.length > 1) {
        const format = args[1].toLowerCase();
        if (format === 'json') {
            console.log(logger.exportJSON());
        } else if (format === 'csv') {
            console.log(logger.exportCSV());
        } else {
            await interact(agentConfig);
            console.error('Unsupported format. Use: json or csv');
            process.exit(-1);
        }
    } else {
        await canary(); // Only run canary if not directly testing sentiment
        if (args.length > 0) {
            args.forEach(evaluate);
        } else {
            const port = parseInt(process.env.HTTP_PORT, 10);
            if (!Number.isNaN(port) && port > 0 && port < 65536) {
                await serve(port);
            } else {
                await interact();
            }
        }
    }
})();
